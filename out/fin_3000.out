[2024-08-21 19:47:02,449] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/home/tiger/.local/lib/python3.9/site-packages/bytedmetrics/__init__.py:10: UserWarning: bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`
  warnings.warn("bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`")
Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ScriptArguments(model_name_or_path='/mnt/bn/data-tns-live-llm/leon/datasets/llama-3-8b-bnb-4bit/', dataset_name='iid2niid_fin_3000', log_with='none', learning_rate=5e-05, batch_size=16, seq_length=2048, gradient_accumulation_steps=2, load_in_8bit=False, load_in_4bit=True, use_peft=True, trust_remote_code=False, output_dir='/mnt/bn/data-tns-live-llm/leon/datasets/fed/iid2niid_fin_3000_20000_fedavg_c10s2_i10_b16a2_l2048_r32a64_f0', peft_lora_r=32, peft_lora_alpha=64, logging_steps=100, use_auth_token=False, num_train_epochs=5, max_steps=10, save_steps=1000, save_total_limit=3, push_to_hub=False, hub_model_id=None, gradient_checkpointing=True, template='alpaca', seed=2023, dpo_beta=0.1, dataset_sample=20000, local_data_dir=None, unsloth=1, bf16=1, online_dataset=0, full_data=0) FedArguments(fed_alg='fedavg', num_rounds=30, num_clients=10, sample_clients=2, split_strategy='iid', prox_mu=0.01, fedopt_tau=0.001, fedopt_eta=0.001, fedopt_beta1=0.9, fedopt_beta2=0.99, save_model_freq=10)
using unsloth model
==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.
   \\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.4.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
30
>> ==================== Round 1 : [6, 9] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:03<00:33,  3.67s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:18,  2.35s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:13,  1.88s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:07<00:10,  1.68s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:10<00:09,  1.90s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:12<00:08,  2.16s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:15<00:06,  2.27s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:18<00:05,  2.50s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:20<00:02,  2.57s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.66s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 26.6353, 'train_samples_per_second': 12.014, 'train_steps_per_second': 0.375, 'train_loss': 1.6622392654418945, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:26,  2.89s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:22,  2.76s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:19,  2.84s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:11<00:16,  2.73s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:14<00:14,  2.86s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:16<00:10,  2.74s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:19<00:08,  2.69s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:22<00:05,  2.98s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:25<00:02,  2.90s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.94s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.94s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.99s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 29.939, 'train_samples_per_second': 10.688, 'train_steps_per_second': 0.334, 'train_loss': 1.4898466110229491, 'epoch': 1.0}
>> ==================== Round 2 : [1, 2] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:15,  1.67s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:03<00:13,  1.73s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:15,  2.18s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:08<00:14,  2.41s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:11<00:12,  2.52s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:14<00:10,  2.55s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:16<00:07,  2.53s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:19<00:04,  2.48s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:21<00:02,  2.60s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.67s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.67s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.82s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 28.229, 'train_samples_per_second': 11.336, 'train_steps_per_second': 0.354, 'train_loss': 0.2727890253067017, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:23,  2.66s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:23,  2.93s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:09<00:22,  3.28s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:12<00:19,  3.28s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:15<00:16,  3.22s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:18<00:11,  2.96s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:21<00:08,  3.00s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:23<00:05,  2.85s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:27<00:02,  2.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  2.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.22s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 32.2422, 'train_samples_per_second': 9.925, 'train_steps_per_second': 0.31, 'train_loss': 0.2440579891204834, 'epoch': 1.0}
>> ==================== Round 3 : [0, 1] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:21,  2.35s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:23,  2.88s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:19,  2.74s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:11<00:18,  3.08s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:14<00:14,  2.91s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:17<00:11,  2.96s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:20<00:08,  2.96s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:23<00:06,  3.10s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:26<00:02,  2.90s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  3.09s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  3.09s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  3.09s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 30.9326, 'train_samples_per_second': 10.345, 'train_steps_per_second': 0.323, 'train_loss': 0.30275650024414064, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:03<00:27,  3.02s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:23,  2.94s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:20,  2.89s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:11<00:17,  2.92s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:14<00:13,  2.78s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:17<00:11,  2.93s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:20<00:08,  2.85s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:22<00:05,  2.78s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:25<00:02,  2.84s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  3.23s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  3.23s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  3.19s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 31.8946, 'train_samples_per_second': 10.033, 'train_steps_per_second': 0.314, 'train_loss': 0.19071463346481324, 'epoch': 1.0}
>> ==================== Round 4 : [3, 8] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:21,  2.41s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:04<00:16,  2.04s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:15,  2.15s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:16,  2.80s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:15,  3.12s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:18<00:13,  3.44s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:21<00:10,  3.41s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:25<00:07,  3.66s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:30<00:04,  4.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:34<00:00,  4.18s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:37<00:00,  4.18s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:37<00:00,  3.73s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 37.328, 'train_samples_per_second': 8.573, 'train_steps_per_second': 0.268, 'train_loss': 0.1692060947418213, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:04<00:37,  4.21s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:08<00:35,  4.39s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:13<00:31,  4.56s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:17<00:25,  4.31s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:21<00:21,  4.35s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:25<00:16,  4.15s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:29<00:11,  3.94s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:33<00:08,  4.08s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:37<00:04,  4.05s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:41<00:00,  3.90s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:43<00:00,  3.90s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:43<00:00,  4.30s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 43.0529, 'train_samples_per_second': 7.433, 'train_steps_per_second': 0.232, 'train_loss': 0.19297852516174316, 'epoch': 1.0}
>> ==================== Round 5 : [3, 4] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:04<00:37,  4.15s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:06<00:24,  3.06s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:18,  2.65s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:14,  2.39s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:12<00:11,  2.25s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:14<00:08,  2.18s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:16<00:06,  2.11s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:18<00:04,  2.06s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:20<00:01,  1.99s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.06s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:25<00:00,  2.06s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:25<00:00,  2.50s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 25.028, 'train_samples_per_second': 12.786, 'train_steps_per_second': 0.4, 'train_loss': 0.1658802032470703, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:19,  2.13s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:03<00:14,  1.86s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:16,  2.41s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:09<00:14,  2.37s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:11<00:11,  2.30s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:13<00:08,  2.15s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:15<00:06,  2.04s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:04,  2.04s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:19<00:02,  2.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:20<00:00,  1.99s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  1.99s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.25s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 22.4699, 'train_samples_per_second': 14.241, 'train_steps_per_second': 0.445, 'train_loss': 0.1549882173538208, 'epoch': 1.0}
>> ==================== Round 6 : [4, 9] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:17,  1.97s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:04<00:17,  2.23s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:14,  2.06s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:08<00:11,  2.00s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:09<00:09,  1.90s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:11<00:07,  1.95s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:14<00:06,  2.01s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:15<00:03,  1.92s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:17<00:01,  1.86s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  1.91s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  1.91s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  2.13s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 21.3331, 'train_samples_per_second': 15.0, 'train_steps_per_second': 0.469, 'train_loss': 0.1751491069793701, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:19,  2.12s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:04<00:17,  2.16s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:14,  2.08s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:07<00:11,  1.93s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:10<00:09,  1.97s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:11<00:07,  1.93s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:14<00:05,  2.00s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:16<00:04,  2.08s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:18<00:02,  2.05s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:20<00:00,  2.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.32s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 23.1612, 'train_samples_per_second': 13.816, 'train_steps_per_second': 0.432, 'train_loss': 0.2656696796417236, 'epoch': 1.0}
>> ==================== Round 7 : [1, 9] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:16,  1.82s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:03<00:15,  1.88s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:05<00:12,  1.83s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:07<00:10,  1.81s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:09<00:09,  1.81s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:11<00:07,  1.84s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:12<00:05,  1.76s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:14<00:03,  1.87s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:17<00:02,  2.13s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  2.14s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  2.14s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  2.13s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 21.3215, 'train_samples_per_second': 15.008, 'train_steps_per_second': 0.469, 'train_loss': 0.14404923915863038, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:13,  1.47s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:03<00:15,  1.89s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:05<00:12,  1.83s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:07<00:11,  1.91s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:09<00:09,  1.83s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:10<00:07,  1.77s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:12<00:05,  1.84s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:14<00:03,  1.82s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:16<00:01,  1.80s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  1.73s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  1.73s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.25s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 22.5329, 'train_samples_per_second': 14.201, 'train_steps_per_second': 0.444, 'train_loss': 0.18569397926330566, 'epoch': 1.0}
>> ==================== Round 8 : [2, 5] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:12,  1.38s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:04<00:20,  2.54s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:16,  2.37s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:08<00:12,  2.11s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:10<00:10,  2.05s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:12<00:08,  2.01s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:14<00:06,  2.06s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:16<00:04,  2.04s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:18<00:02,  2.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:20<00:00,  2.02s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  2.02s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  2.19s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 21.9094, 'train_samples_per_second': 14.606, 'train_steps_per_second': 0.456, 'train_loss': 0.14162927865982056, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:13,  1.54s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:03<00:14,  1.78s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:05<00:11,  1.69s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:08<00:13,  2.23s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:12<00:14,  2.82s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:15<00:12,  3.16s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:20<00:10,  3.51s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:23<00:07,  3.56s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:27<00:03,  3.69s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  3.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.31s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 33.1413, 'train_samples_per_second': 9.656, 'train_steps_per_second': 0.302, 'train_loss': 0.1397242546081543, 'epoch': 1.0}
>> ==================== Round 9 : [3, 5] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:03<00:33,  3.70s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:07<00:31,  3.88s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:09<00:20,  2.93s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:12<00:16,  2.81s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:15<00:15,  3.04s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:19<00:13,  3.36s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:23<00:10,  3.40s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:26<00:06,  3.48s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:30<00:03,  3.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:35<00:00,  3.95s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:41<00:00,  3.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:41<00:00,  4.13s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 41.3031, 'train_samples_per_second': 7.748, 'train_steps_per_second': 0.242, 'train_loss': 0.12463244199752807, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:05<00:51,  5.72s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:08<00:33,  4.23s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:10<00:21,  3.12s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:12<00:16,  2.71s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:15<00:13,  2.69s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:18<00:11,  2.97s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:23<00:10,  3.38s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:27<00:07,  3.60s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:31<00:03,  3.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:34<00:00,  3.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:40<00:00,  3.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:40<00:00,  4.03s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 40.2939, 'train_samples_per_second': 7.942, 'train_steps_per_second': 0.248, 'train_loss': 0.17259231805801392, 'epoch': 1.0}
>> ==================== Round 10 : [5, 7] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:03<00:28,  3.18s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:20,  2.55s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:07<00:16,  2.32s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:15,  2.51s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:14<00:15,  3.07s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:18<00:13,  3.34s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:21<00:10,  3.45s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:25<00:07,  3.67s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:30<00:03,  3.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:34<00:00,  4.06s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:41<00:00,  4.06s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:41<00:00,  4.20s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 41.9774, 'train_samples_per_second': 7.623, 'train_steps_per_second': 0.238, 'train_loss': 0.11374907493591309, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:20,  2.29s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:24,  3.10s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:10<00:25,  3.58s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:13<00:22,  3.68s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:18<00:19,  3.84s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:28<00:24,  6.06s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:32<00:16,  5.37s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:36<00:09,  4.90s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:40<00:04,  4.56s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:42<00:00,  3.82s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:43<00:00,  3.82s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:43<00:00,  4.34s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 43.3684, 'train_samples_per_second': 7.379, 'train_steps_per_second': 0.231, 'train_loss': 0.14836454391479492, 'epoch': 1.0}
>> ==================== Round 11 : [0, 9] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:03<00:28,  3.17s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:07<00:31,  3.95s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:12<00:29,  4.19s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:16<00:24,  4.14s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:20<00:21,  4.30s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:25<00:17,  4.33s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:29<00:12,  4.25s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:33<00:08,  4.23s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:37<00:04,  4.18s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:40<00:00,  3.76s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:41<00:00,  3.76s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:41<00:00,  4.16s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 41.5996, 'train_samples_per_second': 7.692, 'train_steps_per_second': 0.24, 'train_loss': 0.19032517671585084, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:03<00:31,  3.54s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:07<00:31,  3.94s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:11<00:27,  3.99s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:15<00:22,  3.75s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:19<00:19,  3.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:22<00:15,  3.76s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:26<00:11,  3.92s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:30<00:07,  3.88s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:34<00:03,  3.88s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:37<00:00,  3.57s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:43<00:00,  3.57s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:43<00:00,  4.39s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 43.8696, 'train_samples_per_second': 7.294, 'train_steps_per_second': 0.228, 'train_loss': 0.24811828136444092, 'epoch': 1.0}
>> ==================== Round 12 : [7, 8] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:04<00:37,  4.15s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:08<00:33,  4.18s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:12<00:29,  4.18s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:16<00:23,  3.97s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:19<00:19,  3.91s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:23<00:14,  3.61s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:25<00:09,  3.20s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:27<00:05,  2.88s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:29<00:02,  2.73s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  2.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  2.66s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.36s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 33.632, 'train_samples_per_second': 9.515, 'train_steps_per_second': 0.297, 'train_loss': 0.17911311388015747, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:08<01:16,  8.49s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:12<00:46,  5.77s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:16<00:34,  4.98s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:20<00:27,  4.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:24<00:22,  4.40s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:28<00:17,  4.27s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:31<00:11,  3.95s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:33<00:06,  3.35s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:35<00:02,  2.82s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:38<00:00,  2.79s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:39<00:00,  2.79s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:39<00:00,  3.99s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 39.9399, 'train_samples_per_second': 8.012, 'train_steps_per_second': 0.25, 'train_loss': 0.13030223846435546, 'epoch': 1.0}
>> ==================== Round 13 : [4, 7] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:04<00:36,  4.07s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:07<00:30,  3.85s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:10<00:24,  3.55s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:13<00:18,  3.15s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:17<00:17,  3.46s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:20<00:12,  3.15s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:22<00:08,  2.98s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:24<00:05,  2.54s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:25<00:02,  2.17s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.25s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  2.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  3.19s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 31.0012, 'train_samples_per_second': 10.322, 'train_steps_per_second': 0.323, 'train_loss': 0.17170864343643188, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:25,  2.89s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:21,  2.67s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:18,  2.68s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:16,  2.70s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:13,  2.62s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:14<00:08,  2.22s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:16<00:06,  2.19s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:20<00:05,  2.53s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:27<00:04,  4.05s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  3.56s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.56s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.21s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 32.0696, 'train_samples_per_second': 9.978, 'train_steps_per_second': 0.312, 'train_loss': 0.2644156217575073, 'epoch': 1.0}
>> ==================== Round 14 : [4, 9] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:25,  2.88s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:21,  2.68s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:14,  2.09s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:08<00:10,  1.77s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:09<00:07,  1.56s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:10<00:06,  1.52s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:12<00:04,  1.47s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:13<00:02,  1.45s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:15<00:01,  1.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  1.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  1.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  2.14s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 21.3824, 'train_samples_per_second': 14.966, 'train_steps_per_second': 0.468, 'train_loss': 0.10005823373794556, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:23,  2.67s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:22,  2.85s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:20,  2.93s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:11<00:16,  2.83s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:14<00:13,  2.77s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:15<00:08,  2.24s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:16<00:05,  1.89s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:18<00:03,  1.89s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:21<00:02,  2.31s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  3.86s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.86s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.27s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 32.717, 'train_samples_per_second': 9.781, 'train_steps_per_second': 0.306, 'train_loss': 0.18591433763504028, 'epoch': 1.0}
>> ==================== Round 15 : [1, 8] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:23,  2.65s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:22,  2.77s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:14,  2.11s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:07<00:10,  1.73s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:09<00:08,  1.65s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:11<00:07,  1.79s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:14<00:06,  2.08s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:16<00:04,  2.28s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:19<00:02,  2.48s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.39s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 23.9079, 'train_samples_per_second': 13.385, 'train_steps_per_second': 0.418, 'train_loss': 0.14285411834716796, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:22,  2.52s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:22,  2.85s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:07<00:18,  2.64s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:09<00:13,  2.18s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:11<00:10,  2.01s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:12<00:07,  1.80s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:14<00:05,  1.72s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:15<00:03,  1.52s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:16<00:01,  1.44s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:17<00:00,  1.40s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.40s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  1.89s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 18.8657, 'train_samples_per_second': 16.962, 'train_steps_per_second': 0.53, 'train_loss': 0.10884714126586914, 'epoch': 1.0}
>> ==================== Round 16 : [0, 3] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:25,  2.86s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:22,  2.76s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:19,  2.79s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:16,  2.72s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:13,  2.78s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:16<00:10,  2.59s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:18<00:08,  2.67s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:21<00:05,  2.66s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:23<00:02,  2.43s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:25<00:00,  2.21s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.21s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.62s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 26.2483, 'train_samples_per_second': 12.191, 'train_steps_per_second': 0.381, 'train_loss': 0.17342616319656373, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:25,  2.87s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:21,  2.63s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:19,  2.77s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:16,  2.72s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:13,  2.62s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:16<00:10,  2.68s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:18<00:08,  2.73s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:21<00:05,  2.67s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:24<00:02,  2.69s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:25<00:00,  2.33s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.33s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.69s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 26.8625, 'train_samples_per_second': 11.913, 'train_steps_per_second': 0.372, 'train_loss': 0.09042361378669739, 'epoch': 1.0}
>> ==================== Round 17 : [5, 7] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:25,  2.79s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:23,  2.91s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:19,  2.82s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:11<00:16,  2.81s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:13,  2.73s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:16<00:10,  2.63s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:18<00:07,  2.58s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:21<00:05,  2.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:23<00:02,  2.57s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.89s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 28.9344, 'train_samples_per_second': 11.059, 'train_steps_per_second': 0.346, 'train_loss': 0.13777927160263062, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:03<00:28,  3.12s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:23,  2.90s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:19,  2.79s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:11<00:16,  2.69s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:13,  2.65s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:17<00:11,  2.93s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:20<00:08,  2.92s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:22<00:05,  2.88s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:25<00:02,  2.94s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.99s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  2.99s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  3.04s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 30.4113, 'train_samples_per_second': 10.522, 'train_steps_per_second': 0.329, 'train_loss': 0.16352133750915526, 'epoch': 1.0}
>> ==================== Round 18 : [6, 8] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:25,  2.81s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:20,  2.51s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:07<00:18,  2.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:16,  2.67s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:12<00:12,  2.57s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:15<00:10,  2.56s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:18<00:07,  2.58s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:20<00:05,  2.65s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:23<00:02,  2.69s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  2.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.39s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 33.9232, 'train_samples_per_second': 9.433, 'train_steps_per_second': 0.295, 'train_loss': 0.11573915481567383, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:03<00:27,  3.01s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:20,  2.60s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:20,  2.97s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:11<00:16,  2.74s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:13,  2.63s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:16<00:10,  2.71s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:19<00:08,  2.69s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:21<00:05,  2.59s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:24<00:02,  2.59s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.66s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.83s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 28.3154, 'train_samples_per_second': 11.301, 'train_steps_per_second': 0.353, 'train_loss': 0.16508468389511108, 'epoch': 1.0}
>> ==================== Round 19 : [1, 2] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:15,  1.70s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:04<00:16,  2.08s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:16,  2.30s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:09<00:14,  2.40s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:11<00:11,  2.35s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:13<00:09,  2.41s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:16<00:07,  2.47s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:19<00:05,  2.60s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:22<00:02,  2.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.63s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  2.63s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 32.6301, 'train_samples_per_second': 9.807, 'train_steps_per_second': 0.306, 'train_loss': 0.08528437614440917, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:16,  1.82s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:04<00:18,  2.32s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:16,  2.34s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:09<00:14,  2.45s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:12<00:13,  2.76s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:16<00:12,  3.11s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:19<00:08,  2.96s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:21<00:05,  2.85s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:24<00:02,  2.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.85s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  2.85s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  3.11s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 31.154, 'train_samples_per_second': 10.272, 'train_steps_per_second': 0.321, 'train_loss': 0.09479941725730896, 'epoch': 1.0}
>> ==================== Round 20 : [0, 8] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:03<00:34,  3.82s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:06<00:25,  3.23s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:09<00:20,  2.87s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:11<00:16,  2.77s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:15<00:15,  3.01s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:17<00:11,  2.84s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:20<00:08,  2.99s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:23<00:05,  2.86s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:26<00:02,  2.83s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.44s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.44s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.91s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 29.0668, 'train_samples_per_second': 11.009, 'train_steps_per_second': 0.344, 'train_loss': 0.14896371364593505, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:20,  2.32s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:21,  2.64s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:07<00:18,  2.71s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:16,  2.78s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:13,  2.73s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:17<00:12,  3.13s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:20<00:09,  3.09s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:23<00:05,  2.95s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:25<00:02,  2.83s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.48s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.48s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.84s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 28.4377, 'train_samples_per_second': 11.253, 'train_steps_per_second': 0.352, 'train_loss': 0.08506863713264465, 'epoch': 1.0}
>> ==================== Round 21 : [2, 4] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:24,  2.70s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:22,  2.84s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:18,  2.70s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:15,  2.65s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:13,  2.65s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:15<00:10,  2.61s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:18<00:07,  2.54s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:21<00:05,  2.62s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:23<00:02,  2.60s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  2.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.29s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 32.8572, 'train_samples_per_second': 9.739, 'train_steps_per_second': 0.304, 'train_loss': 0.10975501537322999, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:19,  2.13s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:04<00:17,  2.19s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:07<00:17,  2.48s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:09<00:15,  2.61s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:12<00:13,  2.68s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:15<00:10,  2.61s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:17<00:07,  2.61s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:20<00:05,  2.53s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:23<00:02,  2.69s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:25<00:00,  2.62s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.94s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 29.3571, 'train_samples_per_second': 10.9, 'train_steps_per_second': 0.341, 'train_loss': 0.09842592477798462, 'epoch': 1.0}
>> ==================== Round 22 : [2, 6] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:06<00:57,  6.43s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:09<00:34,  4.37s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:11<00:24,  3.53s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:14<00:18,  3.16s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:17<00:15,  3.03s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:19<00:11,  2.89s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:22<00:08,  2.92s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:25<00:05,  2.78s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:27<00:02,  2.67s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  2.69s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  2.69s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  3.20s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 31.9733, 'train_samples_per_second': 10.008, 'train_steps_per_second': 0.313, 'train_loss': 0.10333813428878784, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:12,  1.41s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:02<00:10,  1.37s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:05<00:13,  1.87s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:08<00:14,  2.43s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:11<00:12,  2.56s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:13<00:10,  2.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:16<00:07,  2.64s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:19<00:05,  2.80s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:22<00:02,  2.72s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.69s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.69s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.64s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 26.3676, 'train_samples_per_second': 12.136, 'train_steps_per_second': 0.379, 'train_loss': 0.10474975109100342, 'epoch': 1.0}
>> ==================== Round 23 : [2, 3] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:11,  1.25s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:03<00:16,  2.01s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:05<00:14,  2.06s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:08<00:14,  2.46s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:11<00:12,  2.50s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:14<00:10,  2.61s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:17<00:07,  2.65s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:19<00:05,  2.63s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:23<00:02,  2.93s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.97s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.97s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.78s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 27.8425, 'train_samples_per_second': 11.493, 'train_steps_per_second': 0.359, 'train_loss': 0.1142384171485901, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:10,  1.21s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:03<00:16,  2.06s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:17,  2.44s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:09<00:14,  2.44s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:11<00:12,  2.55s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:14<00:10,  2.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:17<00:07,  2.63s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:19<00:05,  2.56s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:22<00:02,  2.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.60s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.60s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.64s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 26.4056, 'train_samples_per_second': 12.119, 'train_steps_per_second': 0.379, 'train_loss': 0.08674449920654297, 'epoch': 1.0}
>> ==================== Round 24 : [1, 4] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:12,  1.38s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:03<00:16,  2.07s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:17,  2.44s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:09<00:14,  2.48s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:12<00:12,  2.57s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:14<00:10,  2.56s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:17<00:07,  2.61s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:20<00:05,  2.77s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:23<00:02,  2.74s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:25<00:00,  2.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.77s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 27.691, 'train_samples_per_second': 11.556, 'train_steps_per_second': 0.361, 'train_loss': 0.1095767617225647, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:01<00:17,  1.95s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:04<00:16,  2.12s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:06<00:16,  2.35s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:09<00:14,  2.49s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:12<00:12,  2.56s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:14<00:10,  2.54s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:17<00:07,  2.65s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:19<00:05,  2.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:22<00:02,  2.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.51s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  2.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:31<00:00,  3.15s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 31.5067, 'train_samples_per_second': 10.157, 'train_steps_per_second': 0.317, 'train_loss': 0.11182917356491089, 'epoch': 1.0}
>> ==================== Round 25 : [2, 6] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:22,  2.46s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:20,  2.59s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:07<00:18,  2.62s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:16,  2.69s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:13,  2.75s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:15<00:10,  2.62s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:18<00:08,  2.70s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:21<00:05,  2.70s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:23<00:02,  2.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:25<00:00,  2.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.32s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.64s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 26.4289, 'train_samples_per_second': 12.108, 'train_steps_per_second': 0.378, 'train_loss': 0.13980051279067993, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:24,  2.74s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:21,  2.73s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:07<00:18,  2.63s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:16,  2.68s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:13,  2.72s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:16<00:10,  2.73s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:18<00:07,  2.66s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:21<00:05,  2.62s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:23<00:02,  2.33s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.08s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.08s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.78s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 27.7994, 'train_samples_per_second': 11.511, 'train_steps_per_second': 0.36, 'train_loss': 0.12016823291778564, 'epoch': 1.0}
>> ==================== Round 26 : [0, 6] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:03<00:28,  3.19s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:06<00:26,  3.26s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:09<00:21,  3.01s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:11<00:17,  2.89s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:14<00:14,  2.95s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:17<00:11,  2.86s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:19<00:07,  2.47s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:20<00:04,  2.11s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:22<00:01,  1.97s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:24<00:00,  2.16s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.16s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.77s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 27.7323, 'train_samples_per_second': 11.539, 'train_steps_per_second': 0.361, 'train_loss': 0.10136995315551758, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:24,  2.71s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:21,  2.73s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:20,  2.99s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:11<00:17,  2.94s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:13,  2.68s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:15<00:09,  2.28s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:16<00:05,  1.98s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:17<00:03,  1.74s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:20<00:01,  1.89s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  1.84s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  1.84s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.96s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 29.62, 'train_samples_per_second': 10.804, 'train_steps_per_second': 0.338, 'train_loss': 0.07655726075172424, 'epoch': 1.0}
>> ==================== Round 27 : [3, 9] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:22,  2.55s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:04<00:15,  1.91s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:05<00:13,  1.93s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:07<00:10,  1.76s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:08<00:07,  1.60s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:10<00:06,  1.51s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:11<00:04,  1.44s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:14<00:03,  1.90s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:17<00:02,  2.17s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:19<00:00,  2.20s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:20<00:00,  2.20s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:20<00:00,  2.08s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 20.7949, 'train_samples_per_second': 15.388, 'train_steps_per_second': 0.481, 'train_loss': 0.07518284320831299, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:23,  2.60s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:20,  2.61s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:20,  2.91s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:16,  2.67s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:12<00:12,  2.46s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:14<00:08,  2.11s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:15<00:05,  1.89s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:18<00:04,  2.24s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:21<00:02,  2.33s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.75s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 27.514, 'train_samples_per_second': 11.63, 'train_steps_per_second': 0.363, 'train_loss': 0.14851661920547485, 'epoch': 1.0}
>> ==================== Round 28 : [4, 7] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:23,  2.63s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:20,  2.57s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:07<00:18,  2.59s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:09<00:12,  2.13s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:10<00:09,  1.82s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:11<00:06,  1.64s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:14<00:05,  1.87s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:16<00:04,  2.00s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:19<00:02,  2.23s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:21<00:00,  2.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:22<00:00,  2.27s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 22.7329, 'train_samples_per_second': 14.077, 'train_steps_per_second': 0.44, 'train_loss': 0.10504426956176757, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:23,  2.62s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:22,  2.82s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:19,  2.73s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:11<00:16,  2.81s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:12<00:12,  2.44s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:18<00:14,  3.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:21<00:09,  3.27s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:24<00:06,  3.28s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:27<00:03,  3.14s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  2.98s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  2.98s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:33<00:00,  3.39s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 33.8993, 'train_samples_per_second': 9.44, 'train_steps_per_second': 0.295, 'train_loss': 0.1777743935585022, 'epoch': 1.0}
>> ==================== Round 29 : [1, 2] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:26,  2.91s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:04<00:15,  1.98s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:05<00:11,  1.66s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:06<00:08,  1.49s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:07<00:06,  1.34s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:09<00:05,  1.29s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:10<00:04,  1.36s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:11<00:02,  1.37s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:13<00:01,  1.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:14<00:00,  1.35s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.57s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.7172, 'train_samples_per_second': 20.36, 'train_steps_per_second': 0.636, 'train_loss': 0.07400715351104736, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:03<00:28,  3.18s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:22,  2.81s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:18,  2.70s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:16,  2.67s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:13<00:13,  2.62s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:15<00:10,  2.57s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:18<00:07,  2.63s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:21<00:05,  2.55s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:23<00:02,  2.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:25<00:00,  2.29s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.29s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:27<00:00,  2.73s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 27.346, 'train_samples_per_second': 11.702, 'train_steps_per_second': 0.366, 'train_loss': 0.1230291485786438, 'epoch': 1.0}
>> ==================== Round 30 : [1, 8] ====================
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:23,  2.63s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:20,  2.54s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:07<00:18,  2.59s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:10<00:15,  2.64s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:12<00:12,  2.58s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:15<00:10,  2.64s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:18<00:07,  2.66s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:20<00:05,  2.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:23<00:02,  2.59s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.61s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.92s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:accelerate.utils.other:Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 2
\        /    Total batch size = 32 | Total steps = 10
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 29.2314, 'train_samples_per_second': 10.947, 'train_steps_per_second': 0.342, 'train_loss': 0.09513849020004272, 'epoch': 1.0}
  0%|          | 0/10 [00:00<?, ?it/s] 10%|â–ˆ         | 1/10 [00:02<00:24,  2.72s/it] 20%|â–ˆâ–ˆ        | 2/10 [00:05<00:21,  2.71s/it] 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:08<00:21,  3.02s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:11<00:18,  3.05s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:14<00:14,  2.92s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:17<00:11,  2.98s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:20<00:08,  2.84s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:24<00:06,  3.15s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:27<00:03,  3.11s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:29<00:00,  2.96s/it]
{'train_runtime': 29.6405, 'train_samples_per_second': 10.796, 'train_steps_per_second': 0.337, 'train_loss': 0.09067273139953613, 'epoch': 1.0}
