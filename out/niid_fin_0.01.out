/home/tiger/.local/lib/python3.9/site-packages/bytedmetrics/__init__.py:10: UserWarning: bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`
  warnings.warn("bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`")
[2024-08-14 16:40:58,576] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ScriptArguments(model_name_or_path='/mnt/bn/data-tns-live-llm/leon/datasets/llama-3-8b-bnb-4bit/', dataset_name='niid_fin_0.01', log_with='none', learning_rate=5e-05, batch_size=16, seq_length=2048, gradient_accumulation_steps=1, load_in_8bit=False, load_in_4bit=True, use_peft=True, trust_remote_code=False, output_dir='/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_fin_0.01_20000_fedavg_c10s2_i20_b16a1_l2048_r128a256_f0', peft_lora_r=128, peft_lora_alpha=256, logging_steps=100, use_auth_token=False, num_train_epochs=5, max_steps=20, save_steps=1000, save_total_limit=3, push_to_hub=False, hub_model_id=None, gradient_checkpointing=True, template='alpaca', seed=2023, dpo_beta=0.1, dataset_sample=20000, local_data_dir=None, unsloth=1, bf16=1, online_dataset=0, full_data=0) FedArguments(fed_alg='fedavg', num_rounds=30, num_clients=10, sample_clients=2, split_strategy='iid', prox_mu=0.01, fedopt_tau=0.001, fedopt_eta=0.001, fedopt_beta1=0.9, fedopt_beta2=0.99, save_model_freq=10)
using unsloth model
==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.dev0.
   \\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.109 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 9.0. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
30
>> ==================== Round 1 : [6, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:02<00:44,  2.35s/it] 10%|â–ˆ         | 2/20 [00:03<00:29,  1.62s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:20,  1.20s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:04<00:15,  1.02it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:05<00:12,  1.16it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:06<00:11,  1.22it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:06<00:09,  1.31it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:07<00:08,  1.35it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:08<00:07,  1.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:08<00:06,  1.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:09<00:06,  1.32it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:10<00:05,  1.37it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:11<00:05,  1.38it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:11<00:04,  1.40it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:12<00:03,  1.35it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:13<00:02,  1.48it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:13<00:01,  1.53it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:14<00:01,  1.56it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:15<00:00,  1.41it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.42it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:20<00:00,  1.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:20<00:00,  1.03s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 20.5241, 'train_samples_per_second': 15.591, 'train_steps_per_second': 0.974, 'train_loss': 0.9327762603759766, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.49it/s] 10%|â–ˆ         | 2/20 [00:00<00:08,  2.16it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:07,  2.36it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.84it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:09,  1.54it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.54it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.53it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:08,  1.50it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.54it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.52it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.55it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.54it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.54it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.52it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.59it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.56it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.61it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.41it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.1675, 'train_samples_per_second': 22.587, 'train_steps_per_second': 1.412, 'train_loss': 0.961277961730957, 'epoch': 1.0}
>> ==================== Round 2 : [1, 2] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.40it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.50it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:11,  1.54it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.54it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.51it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.50it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.53it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.52it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.51it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.50it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:06,  1.01it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:05,  1.04it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:11<00:04,  1.16it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:12<00:03,  1.19it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.26it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:13<00:01,  1.29it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:14<00:00,  1.37it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.21it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.21it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.20it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.7085, 'train_samples_per_second': 19.152, 'train_steps_per_second': 1.197, 'train_loss': 0.23964309692382812, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.55it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.51it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:11,  1.51it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.51it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.51it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.38it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.48it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:06,  1.30it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:05,  1.35it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.39it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.41it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.39it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.42it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.37it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.45it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.45it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.31it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.2378, 'train_samples_per_second': 21.0, 'train_steps_per_second': 1.313, 'train_loss': 0.23724937438964844, 'epoch': 1.0}
>> ==================== Round 3 : [0, 1] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.79it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.61it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:11,  1.54it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.48it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.51it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.51it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.49it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.48it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.51it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.51it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.51it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.55it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.43it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.35it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.38it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.46it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.52it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.52it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.35it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.8655, 'train_samples_per_second': 21.526, 'train_steps_per_second': 1.345, 'train_loss': 0.19024279117584228, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.42it/s] 10%|â–ˆ         | 2/20 [00:01<00:13,  1.33it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.43it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.48it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.47it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.48it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.50it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.50it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.46it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.50it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.57it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.60it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.57it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.54it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.45it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.50it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.54it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:24<00:00,  1.54it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:24<00:00,  1.21s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 24.2942, 'train_samples_per_second': 13.172, 'train_steps_per_second': 0.823, 'train_loss': 0.15704925060272218, 'epoch': 1.0}
>> ==================== Round 4 : [3, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.85it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.72it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:11,  1.44it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.38it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:11,  1.34it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:10,  1.34it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.40it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.39it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.47it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.48it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.49it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.50it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.42it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.38it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.35it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.35it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.42it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.35it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.38it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.38it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.13it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.7451, 'train_samples_per_second': 18.033, 'train_steps_per_second': 1.127, 'train_loss': 0.1781067967414856, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.31it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.41it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.42it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.45it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.50it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.55it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.52it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.50it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.52it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.51it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.47it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.42it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:05,  1.36it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.47it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.42it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.45it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.47it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.52it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.46it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.49it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.49it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.30it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.3313, 'train_samples_per_second': 20.872, 'train_steps_per_second': 1.305, 'train_loss': 0.17743109464645385, 'epoch': 1.0}
>> ==================== Round 5 : [3, 4] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:16,  1.17it/s] 10%|â–ˆ         | 2/20 [00:01<00:14,  1.24it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:13,  1.23it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:12,  1.32it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.38it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.42it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:08,  1.48it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.46it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:06,  1.45it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.47it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.36it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:05,  1.36it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:04,  1.32it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.37it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.47it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.40it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.34it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.60it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.29it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.5364, 'train_samples_per_second': 20.597, 'train_steps_per_second': 1.287, 'train_loss': 0.16421210765838623, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.24it/s] 10%|â–ˆ         | 2/20 [00:01<00:14,  1.23it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.32it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.39it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.47it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:10,  1.32it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:09,  1.40it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:06,  1.47it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.48it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.56it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.55it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.53it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.48it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.50it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.54it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.52it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.43it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.29it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.4841, 'train_samples_per_second': 20.666, 'train_steps_per_second': 1.292, 'train_loss': 0.15435801744461058, 'epoch': 1.0}
>> ==================== Round 6 : [4, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.38it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.45it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.52it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.53it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.52it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.50it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.52it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.55it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.47it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.52it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.53it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.54it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.54it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.60it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.49it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.48it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.48it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.19it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.8068, 'train_samples_per_second': 19.04, 'train_steps_per_second': 1.19, 'train_loss': 0.12948136329650878, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.21it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.46it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:13,  1.24it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:12,  1.32it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:08,  1.67it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.90it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:06,  2.09it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:05,  2.32it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:04,  2.46it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:03,  2.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:03,  2.69it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:05<00:02,  2.85it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:02,  2.87it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:06<00:02,  2.70it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:06<00:01,  2.78it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:07<00:01,  2.92it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:07<00:01,  2.90it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:07<00:00,  2.74it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:08<00:00,  2.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:08<00:00,  2.67it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.55it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.9289, 'train_samples_per_second': 24.751, 'train_steps_per_second': 1.547, 'train_loss': 0.1498051404953003, 'epoch': 1.0}
>> ==================== Round 7 : [1, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.27it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.43it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.45it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.34it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.41it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.50it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:07,  1.35it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.40it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.38it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:05,  1.36it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.41it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.45it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.48it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.42it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.45it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.51it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.51it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.27it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.8084, 'train_samples_per_second': 20.242, 'train_steps_per_second': 1.265, 'train_loss': 0.14055951833724975, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.57it/s] 10%|â–ˆ         | 2/20 [00:01<00:13,  1.33it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:13,  1.28it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.42it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.45it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.52it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.53it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.51it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:04,  1.87it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:03,  2.05it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  2.28it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:02,  2.48it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:01,  2.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:01,  2.58it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  2.55it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:00,  2.56it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:09<00:00,  2.65it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  2.76it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  2.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.32it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.1252, 'train_samples_per_second': 21.157, 'train_steps_per_second': 1.322, 'train_loss': 0.14610822200775148, 'epoch': 1.0}
>> ==================== Round 8 : [2, 5] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.64it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.55it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:11,  1.48it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.53it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.42it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.47it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.55it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.43it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.45it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.54it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.55it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.48it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.49it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.49it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.53it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.52it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.58it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.55it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.34it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.9446, 'train_samples_per_second': 21.412, 'train_steps_per_second': 1.338, 'train_loss': 0.12114100456237793, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.38it/s] 10%|â–ˆ         | 2/20 [00:01<00:14,  1.23it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.33it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.38it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.47it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.50it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.51it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.52it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.48it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.50it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.42it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.45it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.50it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.50it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.52it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.54it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.57it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.55it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.56it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.55it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.33it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.0466, 'train_samples_per_second': 21.267, 'train_steps_per_second': 1.329, 'train_loss': 0.11798560619354248, 'epoch': 1.0}
>> ==================== Round 9 : [3, 5] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:19,  1.04s/it] 10%|â–ˆ         | 2/20 [00:01<00:16,  1.09it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:14,  1.14it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:13,  1.18it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:11,  1.31it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:10,  1.35it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:08,  1.57it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:06,  1.83it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:05,  2.01it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:04,  2.22it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:03,  2.37it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:03,  2.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:02,  2.55it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:02,  2.66it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:01,  2.58it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:01,  2.72it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:08<00:01,  2.79it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:00,  2.80it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:09<00:00,  2.68it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:09<00:00,  2.65it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.65it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.64it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.2007, 'train_samples_per_second': 26.228, 'train_steps_per_second': 1.639, 'train_loss': 0.11996715068817139, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.42it/s] 10%|â–ˆ         | 2/20 [00:01<00:13,  1.34it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.37it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.45it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.49it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.50it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.39it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.47it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.52it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.44it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.46it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.47it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.52it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.37it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.40it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.39it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.41it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.52it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.52it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.31it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.2555, 'train_samples_per_second': 20.976, 'train_steps_per_second': 1.311, 'train_loss': 0.1212935209274292, 'epoch': 1.0}
>> ==================== Round 10 : [5, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:16,  1.19it/s] 10%|â–ˆ         | 2/20 [00:01<00:14,  1.26it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.37it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.48it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.53it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.54it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.52it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.52it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.56it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.44it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.47it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.49it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.45it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.46it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.50it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.47it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.50it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.60it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.35it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.8222, 'train_samples_per_second': 21.589, 'train_steps_per_second': 1.349, 'train_loss': 0.11111714839935302, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.37it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.74it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  2.03it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:01<00:07,  2.22it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:06,  2.27it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:02<00:05,  2.40it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:05,  2.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:03<00:04,  2.47it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:03<00:04,  2.48it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:04<00:03,  2.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:04<00:03,  2.67it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:04<00:02,  2.83it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:05<00:02,  2.52it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:06<00:02,  2.13it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:06<00:02,  1.78it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:07<00:02,  1.68it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:08<00:01,  1.67it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:08<00:01,  1.61it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:09<00:00,  1.59it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.50it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.50it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.54it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.0055, 'train_samples_per_second': 24.605, 'train_steps_per_second': 1.538, 'train_loss': 0.1157811164855957, 'epoch': 1.0}
>> ==================== Round 11 : [0, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.40it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.52it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.37it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.42it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.48it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.55it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.47it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.50it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.49it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.50it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.53it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.53it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.52it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.56it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.47it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.50it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.55it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.32it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.1614, 'train_samples_per_second': 21.106, 'train_steps_per_second': 1.319, 'train_loss': 0.16155610084533692, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:16,  1.18it/s] 10%|â–ˆ         | 2/20 [00:01<00:13,  1.37it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.49it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.79it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.60it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.59it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.59it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.47it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.38it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.44it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.46it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.40it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.48it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.31it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.40it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.47it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.74it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.01it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  2.01it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.15it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.3662, 'train_samples_per_second': 18.427, 'train_steps_per_second': 1.152, 'train_loss': 0.1221002459526062, 'epoch': 1.0}
>> ==================== Round 12 : [7, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:08,  2.23it/s] 10%|â–ˆ         | 2/20 [00:00<00:08,  2.08it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.79it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.66it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.69it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.61it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.53it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.55it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.48it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.39it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.42it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.37it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.40it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.45it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.47it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.51it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.45it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.45it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.34it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.9435, 'train_samples_per_second': 21.414, 'train_steps_per_second': 1.338, 'train_loss': 0.11104511022567749, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.34it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.47it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.38it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.34it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:11,  1.35it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:10,  1.39it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.50it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.53it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.53it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.48it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.50it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.38it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.40it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.41it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.47it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.42it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.44it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.47it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.10it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 18.1268, 'train_samples_per_second': 17.653, 'train_steps_per_second': 1.103, 'train_loss': 0.13663713932037352, 'epoch': 1.0}
>> ==================== Round 13 : [4, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:08,  2.15it/s] 10%|â–ˆ         | 2/20 [00:00<00:06,  2.65it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:06,  2.73it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:01<00:06,  2.54it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:01<00:05,  2.54it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:02<00:05,  2.70it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:02<00:05,  2.60it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:03<00:04,  2.62it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:03<00:04,  2.61it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:03<00:03,  2.81it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:04<00:03,  2.95it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:04<00:02,  2.93it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:04<00:02,  2.77it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:05<00:02,  2.87it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:05<00:01,  2.98it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:05<00:01,  3.03it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:06<00:00,  3.07it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:06<00:00,  2.81it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:06<00:00,  2.83it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:07<00:00,  2.95it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:08<00:00,  2.95it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:08<00:00,  2.40it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 8.3283, 'train_samples_per_second': 38.423, 'train_steps_per_second': 2.401, 'train_loss': 0.13259704113006593, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.21it/s] 10%|â–ˆ         | 2/20 [00:01<00:14,  1.22it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:13,  1.27it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:13,  1.18it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:12,  1.23it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:10,  1.32it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:09,  1.37it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:08,  1.40it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.40it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:07,  1.35it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:06,  1.40it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.46it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:04,  1.47it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:04,  1.48it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.52it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.46it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.47it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.47it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.49it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.44it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.44it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.09it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 18.4322, 'train_samples_per_second': 17.361, 'train_steps_per_second': 1.085, 'train_loss': 0.11477910280227661, 'epoch': 1.0}
>> ==================== Round 14 : [4, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.28it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.46it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.45it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.47it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.45it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.47it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.50it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.55it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.53it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.50it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.48it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.51it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.41it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.42it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.47it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.41it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.41it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.30it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.42, 'train_samples_per_second': 20.752, 'train_steps_per_second': 1.297, 'train_loss': 0.11516554355621338, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:08,  2.29it/s] 10%|â–ˆ         | 2/20 [00:00<00:07,  2.38it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:06,  2.60it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:01<00:06,  2.64it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:01<00:05,  2.60it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:02<00:05,  2.79it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:02<00:04,  2.67it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:03<00:05,  2.21it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:06,  1.76it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:04<00:06,  1.56it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:05,  1.54it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:05,  1.56it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:04,  1.50it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:04,  1.46it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:03,  1.49it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.48it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:02,  1.50it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.48it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.41it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.41it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.49it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.4457, 'train_samples_per_second': 23.799, 'train_steps_per_second': 1.487, 'train_loss': 0.12022442817687988, 'epoch': 1.0}
>> ==================== Round 15 : [1, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:17,  1.07it/s] 10%|â–ˆ         | 2/20 [00:01<00:16,  1.12it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:14,  1.19it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:12,  1.32it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:11,  1.35it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:05<00:12,  1.10it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:11,  1.15it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:09,  1.27it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:08,  1.36it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:07,  1.34it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:06,  1.38it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:09<00:05,  1.48it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:04,  1.49it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:04,  1.49it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:11<00:03,  1.46it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.48it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:01,  1.53it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:13<00:01,  1.54it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.58it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.26it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.9382, 'train_samples_per_second': 20.078, 'train_steps_per_second': 1.255, 'train_loss': 0.09379764199256897, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.20it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.39it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.49it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.57it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.42it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.46it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.36it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.39it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:07,  1.28it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.36it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:06,  1.31it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:05,  1.30it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:04,  1.35it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.39it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.36it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.42it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:13<00:01,  1.35it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.35it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.45it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.45it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.15it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.3946, 'train_samples_per_second': 18.397, 'train_steps_per_second': 1.15, 'train_loss': 0.11438274383544922, 'epoch': 1.0}
>> ==================== Round 16 : [0, 3] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:02<00:47,  2.52s/it] 10%|â–ˆ         | 2/20 [00:02<00:22,  1.27s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:15,  1.13it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:10,  1.50it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:08,  1.71it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:08,  1.66it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:08,  1.48it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:07,  1.50it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.50it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:06,  1.52it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:05,  1.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.41it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:05,  1.26it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:04,  1.35it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:11<00:03,  1.35it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:12<00:02,  1.37it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.40it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:13<00:01,  1.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:14<00:00,  1.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.49it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.49it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.21it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.5122, 'train_samples_per_second': 19.38, 'train_steps_per_second': 1.211, 'train_loss': 0.11505614519119263, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:16,  1.16it/s] 10%|â–ˆ         | 2/20 [00:01<00:13,  1.31it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.39it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.47it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.52it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:10,  1.40it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.45it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.41it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:08,  1.26it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:07,  1.29it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:07,  1.20it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:09<00:06,  1.27it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:05,  1.38it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:04,  1.35it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:11<00:03,  1.38it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.34it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.41it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:13<00:01,  1.42it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.69it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.69it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.21it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.5512, 'train_samples_per_second': 19.334, 'train_steps_per_second': 1.208, 'train_loss': 0.12105658054351806, 'epoch': 1.0}
>> ==================== Round 17 : [5, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.20it/s] 10%|â–ˆ         | 2/20 [00:01<00:14,  1.25it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.36it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:12,  1.33it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.40it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.42it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.55it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:06,  1.83it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:05,  2.13it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:04,  2.34it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:03,  2.52it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:03,  2.65it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:02,  2.75it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:02,  2.87it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:07<00:01,  2.93it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:07<00:01,  2.93it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:08<00:01,  2.93it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:08<00:00,  2.97it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:08<00:00,  3.01it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:09<00:00,  2.98it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  2.98it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.88it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 10.6711, 'train_samples_per_second': 29.988, 'train_steps_per_second': 1.874, 'train_loss': 0.11320979595184326, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:20,  1.06s/it] 10%|â–ˆ         | 2/20 [00:01<00:15,  1.14it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:13,  1.27it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:11,  1.35it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:11,  1.35it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.40it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:09,  1.41it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.47it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:06,  1.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.43it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.39it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:04,  1.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:04,  1.46it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.51it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.53it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.55it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.53it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.50it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.50it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.27it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.7465, 'train_samples_per_second': 20.322, 'train_steps_per_second': 1.27, 'train_loss': 0.07925685048103333, 'epoch': 1.0}
>> ==================== Round 18 : [6, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:20,  1.08s/it] 10%|â–ˆ         | 2/20 [00:01<00:16,  1.10it/s] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:19,  1.17s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:04<00:16,  1.02s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:05<00:15,  1.01s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:06<00:13,  1.01it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:06<00:12,  1.04it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:07<00:11,  1.07it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:08<00:10,  1.03it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:09<00:09,  1.04it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:10<00:08,  1.12it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:11<00:07,  1.09it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:12<00:06,  1.12it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:13<00:05,  1.09it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:14<00:04,  1.08it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:15<00:03,  1.05it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:16<00:02,  1.11it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:16<00:01,  1.16it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:17<00:00,  1.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.13it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:25<00:00,  1.13it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:25<00:00,  1.30s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 25.9329, 'train_samples_per_second': 12.34, 'train_steps_per_second': 0.771, 'train_loss': 0.07638768553733825, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.34it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.77it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.84it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:07,  2.06it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:07,  2.10it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.42it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:09,  1.31it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:10,  1.08it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:08,  1.14it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:07,  1.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:09<00:07,  1.13it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:10<00:06,  1.08it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:05,  1.10it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:12<00:04,  1.03it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:13<00:04,  1.02s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:14<00:03,  1.03s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:15<00:01,  1.01it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:16<00:00,  1.01it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.03s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:21<00:00,  1.03s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:21<00:00,  1.06s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 21.2782, 'train_samples_per_second': 15.039, 'train_steps_per_second': 0.94, 'train_loss': 0.08426513671875, 'epoch': 1.0}
>> ==================== Round 19 : [1, 2] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:19,  1.03s/it] 10%|â–ˆ         | 2/20 [00:01<00:16,  1.10it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:13,  1.23it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:14,  1.10it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:15,  1.01s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:05<00:12,  1.09it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:06<00:11,  1.10it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:07<00:10,  1.14it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:08<00:09,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:09<00:08,  1.11it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:09<00:07,  1.14it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:10<00:07,  1.11it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:11<00:06,  1.11it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:12<00:05,  1.04it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:13<00:04,  1.14it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:14<00:03,  1.06it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:15<00:02,  1.01it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:16<00:01,  1.08it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:17<00:00,  1.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.05it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:20<00:00,  1.05it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:20<00:00,  1.01s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 20.1237, 'train_samples_per_second': 15.902, 'train_steps_per_second': 0.994, 'train_loss': 0.06633567810058594, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:18,  1.02it/s] 10%|â–ˆ         | 2/20 [00:01<00:15,  1.17it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:14,  1.16it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:13,  1.22it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:12,  1.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:05<00:13,  1.06it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:06<00:11,  1.12it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:10,  1.17it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:09,  1.12it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:08<00:08,  1.21it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:09<00:07,  1.13it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:10<00:07,  1.07it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:11<00:06,  1.12it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:12<00:05,  1.13it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:13<00:04,  1.09it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:14<00:03,  1.12it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:14<00:02,  1.27it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:15<00:01,  1.48it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:15<00:00,  1.54it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.67it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:20<00:00,  1.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:20<00:00,  1.02s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 20.4068, 'train_samples_per_second': 15.681, 'train_steps_per_second': 0.98, 'train_loss': 0.09271334409713745, 'epoch': 1.0}
>> ==================== Round 20 : [0, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:16,  1.14it/s] 10%|â–ˆ         | 2/20 [00:01<00:15,  1.16it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:15,  1.10it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:12,  1.30it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:12,  1.20it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:05<00:11,  1.21it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:10,  1.20it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:10,  1.13it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:10,  1.04it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:08<00:09,  1.04it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:09<00:08,  1.10it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:10<00:07,  1.12it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:11<00:06,  1.09it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:12<00:05,  1.11it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:13<00:04,  1.11it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:14<00:03,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:15<00:02,  1.13it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:16<00:01,  1.08it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:16<00:00,  1.16it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.09it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:19<00:00,  1.09it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:19<00:00,  1.04it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 19.2486, 'train_samples_per_second': 16.625, 'train_steps_per_second': 1.039, 'train_loss': 0.08344695568084717, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:21,  1.15s/it] 10%|â–ˆ         | 2/20 [00:02<00:23,  1.28s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:17,  1.05s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:04<00:16,  1.02s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:05<00:15,  1.06s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:06<00:15,  1.13s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:07<00:14,  1.09s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:08<00:12,  1.05s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:09<00:10,  1.01it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:10<00:10,  1.00s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:11<00:08,  1.07it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:12<00:07,  1.06it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:13<00:06,  1.08it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:14<00:05,  1.11it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:14<00:04,  1.13it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:15<00:03,  1.08it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:16<00:02,  1.11it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:17<00:01,  1.09it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:18<00:00,  1.08it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:19<00:00,  1.08it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:21<00:00,  1.08it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:21<00:00,  1.08s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 21.5066, 'train_samples_per_second': 14.879, 'train_steps_per_second': 0.93, 'train_loss': 0.0884586215019226, 'epoch': 1.0}
>> ==================== Round 21 : [2, 4] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:22,  1.16s/it] 10%|â–ˆ         | 2/20 [00:01<00:16,  1.12it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:15,  1.12it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:15,  1.03it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:14,  1.01it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:05<00:14,  1.01s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:06<00:11,  1.16it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:08,  1.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:07,  1.49it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:05,  1.67it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:05,  1.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:09<00:04,  1.74it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:03,  1.98it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:03,  1.72it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.64it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.40it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.30it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:13<00:01,  1.34it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:14<00:00,  1.27it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.29it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.29it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.17it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.1696, 'train_samples_per_second': 18.638, 'train_steps_per_second': 1.165, 'train_loss': 0.08141738176345825, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:21,  1.12s/it] 10%|â–ˆ         | 2/20 [00:02<00:17,  1.00it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:15,  1.13it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:14,  1.13it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:12,  1.18it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:05<00:12,  1.10it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:06<00:11,  1.13it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:07<00:10,  1.16it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:08<00:09,  1.12it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:08<00:08,  1.16it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:09<00:07,  1.16it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:10<00:06,  1.15it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:11<00:05,  1.20it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:12<00:04,  1.24it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:13<00:04,  1.20it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:13<00:03,  1.27it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:14<00:02,  1.17it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:15<00:01,  1.18it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:16<00:00,  1.20it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.24it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:23<00:00,  1.24it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:23<00:00,  1.20s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 23.9696, 'train_samples_per_second': 13.35, 'train_steps_per_second': 0.834, 'train_loss': 0.09562804698944091, 'epoch': 1.0}
>> ==================== Round 22 : [2, 6] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.25it/s] 10%|â–ˆ         | 2/20 [00:01<00:13,  1.38it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.50it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.50it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.49it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.49it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:10,  1.26it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:09,  1.33it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.40it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:06,  1.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.77it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.96it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:03,  2.23it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:02,  2.39it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  2.49it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:01,  2.60it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  2.67it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:00,  2.59it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  2.70it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  2.72it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.72it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.63it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.2709, 'train_samples_per_second': 26.078, 'train_steps_per_second': 1.63, 'train_loss': 0.09109362959861755, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.23it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.40it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.43it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.53it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.52it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.52it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.42it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.38it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.41it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.45it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.37it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:05,  1.35it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.38it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.36it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.34it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.29it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.38it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.41it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.30it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.30it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.10it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 18.1742, 'train_samples_per_second': 17.607, 'train_steps_per_second': 1.1, 'train_loss': 0.09148419499397278, 'epoch': 1.0}
>> ==================== Round 23 : [2, 3] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.39it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.51it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.49it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.49it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.51it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.59it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.57it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.56it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.46it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.53it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.53it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.52it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.47it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.43it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.47it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.47it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.48it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.52it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.52it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.33it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.0817, 'train_samples_per_second': 21.218, 'train_steps_per_second': 1.326, 'train_loss': 0.08388295173645019, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.85it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.49it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:11,  1.49it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.48it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.41it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:10,  1.35it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.40it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.36it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.39it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.48it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.51it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.50it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:04,  1.41it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.41it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.38it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.41it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.36it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.41it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.37it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.37it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.16it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.2129, 'train_samples_per_second': 18.591, 'train_steps_per_second': 1.162, 'train_loss': 0.07388178706169128, 'epoch': 1.0}
>> ==================== Round 24 : [1, 4] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.25it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.45it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.46it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.38it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.43it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.43it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.47it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.40it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:07,  1.42it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.50it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.51it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.51it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.50it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.55it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.47it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.49it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.49it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.49it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.34it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.9707, 'train_samples_per_second': 21.375, 'train_steps_per_second': 1.336, 'train_loss': 0.06179239153862, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:17,  1.07it/s] 10%|â–ˆ         | 2/20 [00:01<00:13,  1.31it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:13,  1.30it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:12,  1.27it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:11,  1.34it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:10,  1.34it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:09,  1.40it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.45it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.53it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:06,  1.56it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.55it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:04,  1.52it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.53it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.55it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.54it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.54it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.55it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.41it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.37it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.37it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.31it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.2729, 'train_samples_per_second': 20.952, 'train_steps_per_second': 1.31, 'train_loss': 0.08204812407493592, 'epoch': 1.0}
>> ==================== Round 25 : [2, 6] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.80it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.95it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.64it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.49it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.48it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.48it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.49it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.55it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.49it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.42it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.36it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.41it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.46it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.46it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.46it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.48it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.48it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.35it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.865, 'train_samples_per_second': 21.527, 'train_steps_per_second': 1.345, 'train_loss': 0.08409700989723205, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.33it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.44it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.49it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.42it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.46it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.46it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.46it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.51it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.52it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:07,  1.42it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.43it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.41it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.46it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.55it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.52it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.53it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.42it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.36it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.42it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:19<00:00,  1.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:19<00:00,  1.00it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 19.9311, 'train_samples_per_second': 16.055, 'train_steps_per_second': 1.003, 'train_loss': 0.09484359622001648, 'epoch': 1.0}
>> ==================== Round 26 : [0, 6] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.38it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.44it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.46it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.47it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.48it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.59it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:06,  1.87it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:05,  2.12it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:04,  2.36it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:04,  2.26it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:03,  2.42it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:03,  2.56it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:02,  2.70it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:06<00:02,  2.77it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:07<00:01,  2.72it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:07<00:01,  2.84it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:07<00:01,  2.84it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:08<00:00,  2.96it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:08<00:00,  3.00it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:08<00:00,  2.92it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:09<00:00,  2.92it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:09<00:00,  2.00it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 9.9979, 'train_samples_per_second': 32.007, 'train_steps_per_second': 2.0, 'train_loss': 0.09429647922515869, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.20it/s] 10%|â–ˆ         | 2/20 [00:01<00:14,  1.21it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.39it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.45it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.37it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.45it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:09,  1.38it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.47it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.48it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.51it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.46it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.40it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:04,  1.47it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.40it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.44it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.49it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.59it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.44it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:20<00:00,  1.44it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:20<00:00,  1.04s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 20.72, 'train_samples_per_second': 15.444, 'train_steps_per_second': 0.965, 'train_loss': 0.08655117750167847, 'epoch': 1.0}
>> ==================== Round 27 : [3, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.25it/s] 10%|â–ˆ         | 2/20 [00:01<00:14,  1.29it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:13,  1.28it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:11,  1.36it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:11,  1.33it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.42it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:08,  1.47it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.48it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.41it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:06,  1.48it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.39it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.35it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:04,  1.55it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.71it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:02,  1.91it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:01,  2.07it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  2.21it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:00,  2.41it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  2.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  2.51it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  2.51it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.53it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.0681, 'train_samples_per_second': 24.487, 'train_steps_per_second': 1.53, 'train_loss': 0.10168492794036865, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.35it/s] 10%|â–ˆ         | 2/20 [00:01<00:13,  1.31it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:13,  1.28it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.38it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.38it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:10,  1.35it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:09,  1.40it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.38it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:07,  1.36it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.40it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:04,  1.48it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.53it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.52it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.52it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.38it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.40it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.43it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.21it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.5379, 'train_samples_per_second': 19.35, 'train_steps_per_second': 1.209, 'train_loss': 0.07810544371604919, 'epoch': 1.0}
>> ==================== Round 28 : [4, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.24it/s] 10%|â–ˆ         | 2/20 [00:01<00:14,  1.22it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.33it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.43it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.48it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.46it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:09,  1.39it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:09,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:07,  1.26it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:07,  1.24it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:09<00:06,  1.24it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:05,  1.27it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:04,  1.27it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:11<00:03,  1.31it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:12<00:02,  1.37it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.37it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:13<00:01,  1.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:14<00:00,  1.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.48it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.48it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.22it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.366, 'train_samples_per_second': 19.553, 'train_steps_per_second': 1.222, 'train_loss': 0.06312218308448792, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.42it/s] 10%|â–ˆ         | 2/20 [00:01<00:13,  1.33it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.34it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.39it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.41it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.42it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.46it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.47it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.54it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.79it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:03,  2.06it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  2.28it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:02,  2.34it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  2.31it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:01,  2.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  2.46it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:00,  2.59it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  2.62it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  2.75it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  2.75it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.71it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 11.6882, 'train_samples_per_second': 27.378, 'train_steps_per_second': 1.711, 'train_loss': 0.04259234368801117, 'epoch': 1.0}
>> ==================== Round 29 : [1, 2] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:16,  1.19it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.39it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.45it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.49it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.39it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:10,  1.36it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:09,  1.33it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.38it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.39it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:06,  1.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.43it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.51it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:04,  1.51it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.46it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.51it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.46it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.48it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.51it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.51it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.51it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.09it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 18.3488, 'train_samples_per_second': 17.44, 'train_steps_per_second': 1.09, 'train_loss': 0.06210741400718689, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.48it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.49it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.59it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.61it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.60it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.59it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.55it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.54it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.52it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.57it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.47it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:05,  1.40it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.50it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:02,  1.78it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:01,  2.04it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  2.26it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:00,  2.46it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  2.48it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  2.61it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  2.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.27it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.8123, 'train_samples_per_second': 20.237, 'train_steps_per_second': 1.265, 'train_loss': 0.08117530941963196, 'epoch': 1.0}
>> ==================== Round 30 : [1, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.54it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.77it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.67it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.46it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.57it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.49it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.41it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:08,  1.37it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.44it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.45it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.39it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.45it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.41it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.46it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.48it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.49it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.49it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.19it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.8238, 'train_samples_per_second': 19.021, 'train_steps_per_second': 1.189, 'train_loss': 0.043199223279953, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.30it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.40it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.51it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.52it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.54it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.60it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.57it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.55it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.59it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:06,  1.32it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:05,  1.29it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.53it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:02,  1.74it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.88it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  2.11it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:00,  2.23it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  2.21it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  2.25it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.25it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.55it/s]
{'train_runtime': 12.921, 'train_samples_per_second': 24.766, 'train_steps_per_second': 1.548, 'train_loss': 0.0933152437210083, 'epoch': 1.0}
