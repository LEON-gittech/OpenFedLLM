/home/tiger/.local/lib/python3.9/site-packages/bytedmetrics/__init__.py:10: UserWarning: bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`
  warnings.warn("bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`")
[2024-08-14 16:46:15,850] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ScriptArguments(model_name_or_path='/mnt/bn/data-tns-live-llm/leon/datasets/llama-3-8b-bnb-4bit/', dataset_name='niid_fin_10', log_with='none', learning_rate=5e-05, batch_size=16, seq_length=2048, gradient_accumulation_steps=1, load_in_8bit=False, load_in_4bit=True, use_peft=True, trust_remote_code=False, output_dir='/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_fin_10_20000_fedavg_c10s2_i20_b16a1_l2048_r128a256_f0', peft_lora_r=128, peft_lora_alpha=256, logging_steps=100, use_auth_token=False, num_train_epochs=5, max_steps=20, save_steps=1000, save_total_limit=3, push_to_hub=False, hub_model_id=None, gradient_checkpointing=True, template='alpaca', seed=2023, dpo_beta=0.1, dataset_sample=20000, local_data_dir=None, unsloth=1, bf16=1, online_dataset=0, full_data=0) FedArguments(fed_alg='fedavg', num_rounds=30, num_clients=10, sample_clients=2, split_strategy='iid', prox_mu=0.01, fedopt_tau=0.001, fedopt_eta=0.001, fedopt_beta1=0.9, fedopt_beta2=0.99, save_model_freq=10)
using unsloth model
==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.dev0.
   \\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.109 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 9.0. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
30
>> ==================== Round 1 : [6, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:02<00:42,  2.25s/it] 10%|â–ˆ         | 2/20 [00:02<00:20,  1.12s/it] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.34it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:09,  1.73it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:07,  1.96it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:06,  2.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:06,  2.07it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:05,  2.28it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:04,  2.48it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:03,  2.66it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:03,  2.79it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:02,  2.85it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:02,  2.95it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:06<00:02,  2.98it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:07<00:01,  2.77it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:07<00:01,  2.88it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:07<00:01,  2.96it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:08<00:00,  2.96it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:08<00:00,  3.01it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:08<00:00,  2.89it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  2.89it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.93it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 10.368, 'train_samples_per_second': 30.864, 'train_steps_per_second': 1.929, 'train_loss': 0.9948659896850586, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.35it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.39it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.70it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.64it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.54it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.59it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.53it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.57it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.59it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.52it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.50it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.52it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.51it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.54it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.80it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.80it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.47it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.6531, 'train_samples_per_second': 23.438, 'train_steps_per_second': 1.465, 'train_loss': 0.9495070457458497, 'epoch': 1.0}
>> ==================== Round 2 : [1, 2] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.21it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.46it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.54it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.60it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.66it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.68it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.68it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.68it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.52it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.57it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.49it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.55it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.55it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.57it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.53it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.56it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.61it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.38it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.4554, 'train_samples_per_second': 22.137, 'train_steps_per_second': 1.384, 'train_loss': 0.2602758646011353, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:08,  2.11it/s] 10%|â–ˆ         | 2/20 [00:00<00:07,  2.28it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  1.98it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.84it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.75it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.68it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.59it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:06,  1.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.62it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.46it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.54it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.52it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.49it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.54it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.53it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.56it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.61it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.44it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.8843, 'train_samples_per_second': 23.048, 'train_steps_per_second': 1.44, 'train_loss': 0.20114703178405763, 'epoch': 1.0}
>> ==================== Round 3 : [0, 1] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.57it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.49it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:11,  1.53it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.51it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.52it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.46it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.50it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.52it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.55it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.56it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.50it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.55it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.57it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.50it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.46it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.61it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.36it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.7148, 'train_samples_per_second': 21.747, 'train_steps_per_second': 1.359, 'train_loss': 0.1986743688583374, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.49it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.55it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.55it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.59it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.61it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.67it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.65it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.65it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.66it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.65it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.69it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.68it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.70it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.66it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.66it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.66it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.67it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.66it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.66it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.66it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.32it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.1258, 'train_samples_per_second': 21.156, 'train_steps_per_second': 1.322, 'train_loss': 0.19924885034561157, 'epoch': 1.0}
>> ==================== Round 4 : [3, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.44it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.47it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:11,  1.54it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.60it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.60it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.62it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.55it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.59it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.55it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.55it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.59it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.61it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.60it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.78it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.76it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.62it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.62it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.45it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.8118, 'train_samples_per_second': 23.169, 'train_steps_per_second': 1.448, 'train_loss': 0.18385069370269774, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:09,  2.09it/s] 10%|â–ˆ         | 2/20 [00:00<00:07,  2.32it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  2.00it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.68it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:09,  1.66it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.55it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.49it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:08,  1.45it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.43it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.55it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.46it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.53it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.46it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.50it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.54it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.50it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.46it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.50it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.53it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:19<00:00,  1.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:19<00:00,  1.03it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 19.4437, 'train_samples_per_second': 16.458, 'train_steps_per_second': 1.029, 'train_loss': 0.19003350734710694, 'epoch': 1.0}
>> ==================== Round 5 : [3, 4] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.20it/s] 10%|â–ˆ         | 2/20 [00:01<00:13,  1.30it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.32it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:11,  1.34it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:11,  1.36it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:08,  1.48it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.53it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.56it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.59it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:04,  1.60it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.60it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.61it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.61it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.61it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.80it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  2.05it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.19it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  2.19it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.48it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.5243, 'train_samples_per_second': 23.661, 'train_steps_per_second': 1.479, 'train_loss': 0.1542137622833252, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.58it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.65it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.61it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.56it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.61it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.60it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.60it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.56it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.50it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.56it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.65it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.65it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.66it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.63it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.63it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.31it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.2812, 'train_samples_per_second': 20.941, 'train_steps_per_second': 1.309, 'train_loss': 0.14045087099075318, 'epoch': 1.0}
>> ==================== Round 6 : [4, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:07,  2.53it/s] 10%|â–ˆ         | 2/20 [00:00<00:07,  2.41it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  2.00it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.87it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.77it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.72it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:08,  1.58it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.52it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.48it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.45it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.51it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.59it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.50it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:04,  1.41it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.46it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.49it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.52it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.32it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.32it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.41it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.41it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.39it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.3849, 'train_samples_per_second': 22.246, 'train_steps_per_second': 1.39, 'train_loss': 0.16439692974090575, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:06,  2.75it/s] 10%|â–ˆ         | 2/20 [00:00<00:06,  2.86it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  1.97it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.80it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.73it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.70it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:08,  1.61it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.65it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.66it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:06,  1.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.60it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:03,  1.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.63it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.62it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.54it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.56it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.50it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.50it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.11it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.979, 'train_samples_per_second': 17.798, 'train_steps_per_second': 1.112, 'train_loss': 0.14062490463256835, 'epoch': 1.0}
>> ==================== Round 7 : [1, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:08,  2.37it/s] 10%|â–ˆ         | 2/20 [00:00<00:06,  2.58it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:07,  2.19it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:01<00:07,  2.04it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:07,  1.88it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.83it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.80it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.76it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:06,  1.78it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:06,  1.64it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.65it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.66it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.67it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.68it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.74it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.61it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.58it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.51it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.2331, 'train_samples_per_second': 24.182, 'train_steps_per_second': 1.511, 'train_loss': 0.12177518606185914, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.68it/s] 10%|â–ˆ         | 2/20 [00:00<00:08,  2.12it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:07,  2.38it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:01<00:08,  1.97it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.74it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.71it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.67it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.66it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:06,  1.66it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.58it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.61it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.54it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.58it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.51it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.53it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.54it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.47it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.45it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.7813, 'train_samples_per_second': 23.22, 'train_steps_per_second': 1.451, 'train_loss': 0.12780237197875977, 'epoch': 1.0}
>> ==================== Round 8 : [2, 5] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:22,  1.19s/it] 10%|â–ˆ         | 2/20 [00:01<00:14,  1.21it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:10,  1.59it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.94it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.60it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.65it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.66it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.66it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.65it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.62it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.56it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.60it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.66it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.67it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.68it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.70it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.71it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.71it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.42it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.0597, 'train_samples_per_second': 22.76, 'train_steps_per_second': 1.423, 'train_loss': 0.14090460538864136, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:08,  2.18it/s] 10%|â–ˆ         | 2/20 [00:00<00:07,  2.44it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  2.05it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.86it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.74it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.68it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.59it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.62it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.62it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:06,  1.64it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.55it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.56it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.53it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:04,  1.49it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.45it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.49it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:02,  1.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.48it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.56it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.56it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.43it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.9843, 'train_samples_per_second': 22.883, 'train_steps_per_second': 1.43, 'train_loss': 0.1293131709098816, 'epoch': 1.0}
>> ==================== Round 9 : [3, 5] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.84it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.87it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.56it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.48it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.53it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.45it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.51it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.55it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.47it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.50it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.53it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.55it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.56it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.58it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.59it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.59it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.52it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.57it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.37it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.6402, 'train_samples_per_second': 21.858, 'train_steps_per_second': 1.366, 'train_loss': 0.11889814138412476, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.60it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.58it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.66it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.46it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.54it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.55it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.38it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.39it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.47it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.51it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.48it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.37it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:05,  1.28it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.24it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.27it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:03,  1.33it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.36it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.36it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.38it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.38it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.38it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.14it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.6234, 'train_samples_per_second': 18.158, 'train_steps_per_second': 1.135, 'train_loss': 0.15161619186401368, 'epoch': 1.0}
>> ==================== Round 10 : [5, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.70it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.58it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.43it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:12,  1.29it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:11,  1.33it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:10,  1.30it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:09,  1.36it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.34it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:08,  1.32it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:07,  1.37it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:06,  1.38it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.37it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:05,  1.39it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:04,  1.27it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:11<00:03,  1.28it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:03,  1.28it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:02,  1.27it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:13<00:01,  1.27it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:14<00:00,  1.23it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.21it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:20<00:00,  1.21it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:20<00:00,  1.04s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 20.8756, 'train_samples_per_second': 15.329, 'train_steps_per_second': 0.958, 'train_loss': 0.1312326669692993, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:20,  1.07s/it] 10%|â–ˆ         | 2/20 [00:01<00:14,  1.24it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:15,  1.13it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:13,  1.23it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:12,  1.17it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:05<00:11,  1.23it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:10,  1.26it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:09,  1.23it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:08,  1.30it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:08<00:08,  1.23it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:06,  1.32it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:09<00:06,  1.20it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:10<00:05,  1.23it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:11<00:04,  1.22it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:12<00:04,  1.13it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:13<00:03,  1.06it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:14<00:02,  1.15it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:15<00:01,  1.13it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:15<00:00,  1.35it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.46it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.46it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.10it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 18.2619, 'train_samples_per_second': 17.523, 'train_steps_per_second': 1.095, 'train_loss': 0.14709120988845825, 'epoch': 1.0}
>> ==================== Round 11 : [0, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:16,  1.14it/s] 10%|â–ˆ         | 2/20 [00:01<00:15,  1.20it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:16,  1.05it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:14,  1.12it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:13,  1.15it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:05<00:10,  1.28it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:06<00:11,  1.18it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:08,  1.34it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:08,  1.24it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:08<00:09,  1.04it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:09<00:08,  1.04it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:10<00:07,  1.12it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:11<00:06,  1.09it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:12<00:05,  1.14it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:13<00:04,  1.18it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:13<00:03,  1.20it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:14<00:02,  1.24it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:15<00:01,  1.28it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:15<00:00,  1.50it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.77it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.15it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.4014, 'train_samples_per_second': 18.389, 'train_steps_per_second': 1.149, 'train_loss': 0.12111881971359253, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.24it/s] 10%|â–ˆ         | 2/20 [00:01<00:15,  1.14it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:13,  1.25it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:12,  1.29it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:13,  1.14it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:11,  1.27it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:10,  1.20it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:09,  1.24it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:08,  1.30it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:08<00:08,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:07,  1.24it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:09<00:06,  1.27it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:10<00:05,  1.24it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:11<00:04,  1.34it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:12<00:03,  1.29it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:12<00:03,  1.33it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:13<00:02,  1.24it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:14<00:01,  1.30it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:15<00:00,  1.13it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.15it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:23<00:00,  1.15it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:23<00:00,  1.20s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 23.919, 'train_samples_per_second': 13.378, 'train_steps_per_second': 0.836, 'train_loss': 0.08414013981819153, 'epoch': 1.0}
>> ==================== Round 12 : [7, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:21,  1.15s/it] 10%|â–ˆ         | 2/20 [00:02<00:17,  1.02it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:15,  1.10it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:12,  1.25it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:12,  1.18it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:05<00:11,  1.27it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:10,  1.25it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:09,  1.31it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:08,  1.25it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:08<00:08,  1.23it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:06,  1.42it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:09<00:04,  1.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:04,  1.67it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:03,  1.86it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:02,  1.84it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.79it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  2.02it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.80it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.97it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  2.10it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  2.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.39it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.4092, 'train_samples_per_second': 22.208, 'train_steps_per_second': 1.388, 'train_loss': 0.11419591903686524, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.29it/s] 10%|â–ˆ         | 2/20 [00:01<00:15,  1.13it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:14,  1.14it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:12,  1.23it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:13,  1.13it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:05<00:11,  1.23it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:10,  1.24it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:09,  1.24it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:08,  1.22it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:08<00:08,  1.13it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:09<00:07,  1.21it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:09<00:06,  1.21it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:10<00:06,  1.15it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:11<00:05,  1.16it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:12<00:04,  1.18it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:13<00:02,  1.40it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:13<00:01,  1.56it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:13<00:01,  1.79it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:14<00:00,  1.89it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.97it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.97it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.25it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.9459, 'train_samples_per_second': 20.068, 'train_steps_per_second': 1.254, 'train_loss': 0.1309584856033325, 'epoch': 1.0}
>> ==================== Round 13 : [4, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.30it/s] 10%|â–ˆ         | 2/20 [00:01<00:17,  1.06it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:14,  1.20it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:13,  1.22it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:11,  1.30it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:11,  1.22it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:10,  1.20it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:09,  1.30it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:08,  1.23it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:08<00:08,  1.17it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:07,  1.26it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:09<00:06,  1.21it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:10<00:05,  1.32it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:11<00:04,  1.26it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:12<00:04,  1.20it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:12<00:03,  1.25it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:13<00:02,  1.35it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:13<00:01,  1.61it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:14<00:00,  1.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.74it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.74it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.22it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.3477, 'train_samples_per_second': 19.575, 'train_steps_per_second': 1.223, 'train_loss': 0.08954780101776123, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.21it/s] 10%|â–ˆ         | 2/20 [00:01<00:14,  1.25it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:15,  1.07it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:13,  1.16it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:04<00:12,  1.24it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:11,  1.24it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:06<00:11,  1.12it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:07<00:11,  1.08it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:09,  1.17it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:08<00:08,  1.16it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:09<00:08,  1.12it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:10<00:06,  1.22it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:11<00:06,  1.16it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:12<00:05,  1.17it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:12<00:04,  1.20it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:13<00:03,  1.25it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:14<00:02,  1.31it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:14<00:01,  1.34it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:15<00:00,  1.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.69it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.69it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.17it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.1134, 'train_samples_per_second': 18.699, 'train_steps_per_second': 1.169, 'train_loss': 0.10095840692520142, 'epoch': 1.0}
>> ==================== Round 14 : [4, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.31it/s] 10%|â–ˆ         | 2/20 [00:01<00:13,  1.34it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:13,  1.30it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:12,  1.28it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:11,  1.26it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:10,  1.29it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:10,  1.23it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:06<00:09,  1.25it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:08,  1.29it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:07<00:07,  1.36it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:06,  1.37it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:09<00:05,  1.34it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:09<00:05,  1.31it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:04,  1.31it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:11<00:03,  1.26it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:12<00:03,  1.17it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:13<00:02,  1.20it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:14<00:01,  1.28it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:14<00:00,  1.33it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.40it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.40it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.19it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.8292, 'train_samples_per_second': 19.015, 'train_steps_per_second': 1.188, 'train_loss': 0.1341493844985962, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.48it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.43it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.40it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.47it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.48it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.45it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.43it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.41it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.47it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.52it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.57it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.61it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.50it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.55it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.59it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.55it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.71it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.71it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.26it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.9252, 'train_samples_per_second': 20.094, 'train_steps_per_second': 1.256, 'train_loss': 0.09717643857002259, 'epoch': 1.0}
>> ==================== Round 15 : [1, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.45it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.60it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:11,  1.49it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.53it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.55it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.59it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.58it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.60it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.62it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.50it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.56it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.62it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.65it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.68it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.94it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.94it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.24it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.1698, 'train_samples_per_second': 19.79, 'train_steps_per_second': 1.237, 'train_loss': 0.11357600688934326, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.58it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.62it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.61it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.66it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.65it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.67it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.67it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.58it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.50it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.56it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.58it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:05,  1.38it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.36it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.38it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.46it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.51it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.56it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.64it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.34it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.9395, 'train_samples_per_second': 21.42, 'train_steps_per_second': 1.339, 'train_loss': 0.09025559425354004, 'epoch': 1.0}
>> ==================== Round 16 : [0, 3] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.57it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.63it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.67it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.54it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.55it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.46it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.51it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.48it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.43it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.55it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.57it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.51it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.49it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.43it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.42it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.50it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.50it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.59it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.59it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.39it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.3847, 'train_samples_per_second': 22.246, 'train_steps_per_second': 1.39, 'train_loss': 0.09643046259880066, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.35it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.53it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.46it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:12,  1.29it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.41it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.52it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.55it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.57it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.52it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.45it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.51it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.46it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.54it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.57it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.60it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.52it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.56it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.59it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.92it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.92it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.44it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.9342, 'train_samples_per_second': 22.965, 'train_steps_per_second': 1.435, 'train_loss': 0.09730095863342285, 'epoch': 1.0}
>> ==================== Round 17 : [5, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.57it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.45it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:11,  1.54it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.46it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.50it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.46it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.51it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.56it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.60it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.59it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.39it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.47it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.50it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.54it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.58it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.52it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.04it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  2.04it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.46it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.703, 'train_samples_per_second': 23.353, 'train_steps_per_second': 1.46, 'train_loss': 0.09734398126602173, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.46it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.42it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.38it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.46it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.42it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.40it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.48it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.49it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.53it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.50it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.54it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.59it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.62it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.62it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.53it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.85it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.10it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  2.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.46it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.7191, 'train_samples_per_second': 23.325, 'train_steps_per_second': 1.458, 'train_loss': 0.13786349296569825, 'epoch': 1.0}
>> ==================== Round 18 : [6, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.44it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.56it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.45it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.57it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.48it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.56it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.47it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.51it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:07,  1.47it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.51it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.53it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.45it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.49it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.50it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.56it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.71it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.94it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  2.18it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.43it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  2.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.52it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.1995, 'train_samples_per_second': 24.243, 'train_steps_per_second': 1.515, 'train_loss': 0.10455809831619263, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.51it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.55it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.61it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.56it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.50it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.55it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.56it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.58it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.53it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.47it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.53it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.56it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.50it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.83it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.83it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.44it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.8491, 'train_samples_per_second': 23.106, 'train_steps_per_second': 1.444, 'train_loss': 0.10430164337158203, 'epoch': 1.0}
>> ==================== Round 19 : [1, 2] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.36it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.51it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.51it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.77it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:07,  2.00it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:06,  2.05it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.83it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.76it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.70it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.69it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.71it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.68it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:03,  1.67it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.68it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.61it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.63it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.63it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.63it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.17it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.162, 'train_samples_per_second': 18.646, 'train_steps_per_second': 1.165, 'train_loss': 0.08171530365943909, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.56it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.62it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.44it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.52it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.48it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.53it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.49it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.58it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.60it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.59it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.62it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.61it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.62it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.59it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.62it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.62it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.64it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.43it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.9832, 'train_samples_per_second': 22.885, 'train_steps_per_second': 1.43, 'train_loss': 0.12204928398132324, 'epoch': 1.0}
>> ==================== Round 20 : [0, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:09,  2.05it/s] 10%|â–ˆ         | 2/20 [00:00<00:08,  2.12it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.79it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.78it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.79it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.74it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.73it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.57it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.51it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:06,  1.56it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:06,  1.49it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.56it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.59it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.51it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.56it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.61it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.61it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.30it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.3729, 'train_samples_per_second': 20.816, 'train_steps_per_second': 1.301, 'train_loss': 0.08746267557144165, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:07,  2.63it/s] 10%|â–ˆ         | 2/20 [00:00<00:07,  2.52it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  2.04it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.85it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:09,  1.67it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.56it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.56it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.57it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.61it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:06,  1.55it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.61it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.65it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.67it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.59it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.52it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.58it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.60it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.53it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.65it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.65it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.48it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.5225, 'train_samples_per_second': 23.664, 'train_steps_per_second': 1.479, 'train_loss': 0.09981610178947449, 'epoch': 1.0}
>> ==================== Round 21 : [2, 4] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:07,  2.56it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.85it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.75it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.73it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:09,  1.61it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.64it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.65it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.66it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.69it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.68it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.66it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.67it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.52it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.48it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.55it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.56it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.60it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.35it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.35it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.43it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.0071, 'train_samples_per_second': 22.846, 'train_steps_per_second': 1.428, 'train_loss': 0.1185415506362915, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:09,  2.10it/s] 10%|â–ˆ         | 2/20 [00:00<00:08,  2.12it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  1.99it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.70it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.67it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.58it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.59it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.65it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.60it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.62it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.62it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.56it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.59it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.62it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.62it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.48it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.5179, 'train_samples_per_second': 23.672, 'train_steps_per_second': 1.48, 'train_loss': 0.11035479307174682, 'epoch': 1.0}
>> ==================== Round 22 : [2, 6] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.32it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.50it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.56it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.47it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.53it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.58it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.61it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.55it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.58it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.51it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.55it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.51it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.59it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.52it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.59it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.55it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.14it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.5014, 'train_samples_per_second': 18.284, 'train_steps_per_second': 1.143, 'train_loss': 0.1102259874343872, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.53it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.59it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.64it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.63it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.64it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.63it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.63it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.65it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.65it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.57it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.57it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.56it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.52it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.49it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.54it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.57it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.58it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.65it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.82it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.16it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.3042, 'train_samples_per_second': 18.493, 'train_steps_per_second': 1.156, 'train_loss': 0.08575673699378968, 'epoch': 1.0}
>> ==================== Round 23 : [2, 3] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.43it/s] 10%|â–ˆ         | 2/20 [00:01<00:15,  1.17it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.37it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.42it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.54it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:08,  1.58it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.63it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.59it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.61it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.55it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.48it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.55it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.60it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.51it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.57it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.75it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.99it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  2.10it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  2.34it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  2.34it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.32it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.1535, 'train_samples_per_second': 21.117, 'train_steps_per_second': 1.32, 'train_loss': 0.09221620559692383, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.40it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.53it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.46it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.51it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.56it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.62it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.62it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.61it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.54it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.55it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.49it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.52it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.48it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:03,  1.15it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.36it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.62it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.01it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  2.01it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.45it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.775, 'train_samples_per_second': 23.23, 'train_steps_per_second': 1.452, 'train_loss': 0.09686314463615417, 'epoch': 1.0}
>> ==================== Round 24 : [1, 4] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:29,  1.57s/it] 10%|â–ˆ         | 2/20 [00:02<00:17,  1.01it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:13,  1.23it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:11,  1.39it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.47it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.53it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:05<00:08,  1.60it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.61it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:06,  1.61it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.65it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:04,  1.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.62it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.52it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.65it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.92it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  2.11it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:00,  2.36it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  2.26it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  2.45it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  2.45it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.54it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.029, 'train_samples_per_second': 24.561, 'train_steps_per_second': 1.535, 'train_loss': 0.06519836187362671, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.42it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.58it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.59it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.63it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.61it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.56it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.54it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.46it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.53it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.46it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.42it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.40it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.45it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.52it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.54it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.59it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.80it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  2.04it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.29it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  2.29it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.38it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.5132, 'train_samples_per_second': 22.049, 'train_steps_per_second': 1.378, 'train_loss': 0.10246764421463013, 'epoch': 1.0}
>> ==================== Round 25 : [2, 6] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.64it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.69it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.72it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.74it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.71it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.69it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.56it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.61it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.61it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.61it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.54it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.47it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:04,  1.42it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.42it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.48it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.42it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.39it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.46it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.55it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.39it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.395, 'train_samples_per_second': 22.23, 'train_steps_per_second': 1.389, 'train_loss': 0.09418158531188965, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.48it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.57it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.60it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.58it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.59it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.65it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.68it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.57it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.50it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.55it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.51it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.56it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.51it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.55it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.51it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.45it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.52it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.77it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.43it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.9911, 'train_samples_per_second': 22.872, 'train_steps_per_second': 1.429, 'train_loss': 0.07322816848754883, 'epoch': 1.0}
>> ==================== Round 26 : [0, 6] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.59it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.63it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:11,  1.51it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.54it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.58it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.50it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.54it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.49it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.52it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.55it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.60it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.54it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.50it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.46it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.51it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.57it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.65it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.92it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.07it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  2.07it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.47it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.5953, 'train_samples_per_second': 23.538, 'train_steps_per_second': 1.471, 'train_loss': 0.06105746626853943, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.36it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.50it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.39it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.37it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.47it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.54it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.58it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.63it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.52it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.60it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.65it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.65it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.66it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.56it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:02,  1.50it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.72it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.91it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.16it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  2.16it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.50it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.356, 'train_samples_per_second': 23.959, 'train_steps_per_second': 1.497, 'train_loss': 0.09352609515190125, 'epoch': 1.0}
>> ==================== Round 27 : [3, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.20it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.45it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.40it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.39it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.48it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.53it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.54it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.56it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.59it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.60it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.61it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.59it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.59it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.62it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.65it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.73it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.96it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.96it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.46it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.6881, 'train_samples_per_second': 23.378, 'train_steps_per_second': 1.461, 'train_loss': 0.08667267560958862, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.51it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.40it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:11,  1.50it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.56it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.62it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.63it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.61it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.63it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.62it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.60it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.57it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.60it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.61it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.55it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.88it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  2.12it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  2.12it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.51it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.2162, 'train_samples_per_second': 24.213, 'train_steps_per_second': 1.513, 'train_loss': 0.08108617067337036, 'epoch': 1.0}
>> ==================== Round 28 : [4, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.42it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.58it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.65it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.46it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.50it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.45it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.52it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.57it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.67it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.67it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.58it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.50it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.56it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.60it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.52it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.47it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.84it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.84it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.45it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.7574, 'train_samples_per_second': 23.26, 'train_steps_per_second': 1.454, 'train_loss': 0.06171404719352722, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.56it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.65it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.56it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.48it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.42it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:09,  1.40it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:08,  1.35it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:06<00:08,  1.37it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.45it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.52it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.46it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:04,  1.47it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:10<00:03,  1.52it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.55it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:11<00:01,  1.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.50it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:12<00:00,  1.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.87it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.87it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.33it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.0295, 'train_samples_per_second': 21.291, 'train_steps_per_second': 1.331, 'train_loss': 0.06991077661514282, 'epoch': 1.0}
>> ==================== Round 29 : [1, 2] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.32it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.53it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.40it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:11,  1.39it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.49it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.52it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.57it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.52it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.54it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:05,  1.57it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.57it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.59it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.60it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.61it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.62it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.65it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.76it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.91it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.14it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  2.14it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.09it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 18.3119, 'train_samples_per_second': 17.475, 'train_steps_per_second': 1.092, 'train_loss': 0.0375973105430603, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:09,  2.06it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.89it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.83it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.78it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.75it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.71it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.70it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.56it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:06,  1.48it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:05,  1.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.54it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.59it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.52it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.58it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.49it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.49it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.15it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.4565, 'train_samples_per_second': 18.331, 'train_steps_per_second': 1.146, 'train_loss': 0.09833533167839051, 'epoch': 1.0}
>> ==================== Round 30 : [1, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.43it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.58it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.57it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.59it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.59it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.62it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.65it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.65it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.65it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:05,  1.69it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.67it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.68it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.68it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.68it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.59it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.54it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.62it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.65it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.81it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.10it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 18.1867, 'train_samples_per_second': 17.595, 'train_steps_per_second': 1.1, 'train_loss': 0.049223136901855466, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.41it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.52it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:12,  1.33it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.48it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:10,  1.45it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:04<00:09,  1.52it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.60it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:05<00:07,  1.53it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:07,  1.56it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.51it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:07<00:06,  1.44it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:08<00:05,  1.52it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:08<00:04,  1.59it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:09<00:03,  1.61it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:09<00:03,  1.67it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:10<00:02,  1.66it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.68it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:11<00:01,  1.81it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  2.02it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  2.24it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  2.24it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.33it/s]
{'train_runtime': 15.0384, 'train_samples_per_second': 21.279, 'train_steps_per_second': 1.33, 'train_loss': 0.11290493011474609, 'epoch': 1.0}
