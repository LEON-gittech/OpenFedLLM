/home/tiger/.local/lib/python3.9/site-packages/bytedmetrics/__init__.py:10: UserWarning: bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`
  warnings.warn("bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`")
[2024-08-14 16:40:44,327] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ScriptArguments(model_name_or_path='/mnt/bn/data-tns-live-llm/leon/datasets/llama-3-8b-bnb-4bit/', dataset_name='niid_math_0.01', log_with='none', learning_rate=5e-05, batch_size=16, seq_length=2048, gradient_accumulation_steps=1, load_in_8bit=False, load_in_4bit=True, use_peft=True, trust_remote_code=False, output_dir='/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_math_0.01_20000_fedavg_c10s2_i20_b16a1_l2048_r128a256_f0', peft_lora_r=128, peft_lora_alpha=256, logging_steps=100, use_auth_token=False, num_train_epochs=5, max_steps=20, save_steps=1000, save_total_limit=3, push_to_hub=False, hub_model_id=None, gradient_checkpointing=True, template='alpaca', seed=2023, dpo_beta=0.1, dataset_sample=20000, local_data_dir=None, unsloth=1, bf16=1, online_dataset=0, full_data=0) FedArguments(fed_alg='fedavg', num_rounds=30, num_clients=10, sample_clients=2, split_strategy='iid', prox_mu=0.01, fedopt_tau=0.001, fedopt_eta=0.001, fedopt_beta1=0.9, fedopt_beta2=0.99, save_model_freq=10)
using unsloth model
==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.dev0.
   \\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.109 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 9.0. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
30
>> ==================== Round 1 : [6, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:07<02:21,  7.45s/it] 10%|â–ˆ         | 2/20 [00:09<01:15,  4.18s/it] 15%|â–ˆâ–Œ        | 3/20 [00:10<00:48,  2.86s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:11<00:34,  2.18s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:13<00:28,  1.89s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:14<00:24,  1.78s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:16<00:21,  1.66s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:17<00:17,  1.46s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:18<00:16,  1.48s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:20<00:14,  1.44s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:22<00:17,  1.90s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:24<00:15,  1.93s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:27<00:14,  2.05s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:29<00:13,  2.18s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:31<00:10,  2.17s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:33<00:07,  1.93s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:34<00:05,  1.75s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:37<00:04,  2.00s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:38<00:01,  1.91s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:39<00:00,  1.63s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:41<00:00,  1.63s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:41<00:00,  2.05s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 41.0345, 'train_samples_per_second': 7.798, 'train_steps_per_second': 0.487, 'train_loss': 0.759210205078125, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:03<01:11,  3.74s/it] 10%|â–ˆ         | 2/20 [00:05<00:41,  2.32s/it] 15%|â–ˆâ–Œ        | 3/20 [00:08<00:48,  2.82s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:10<00:40,  2.51s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:13<00:37,  2.52s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:15<00:33,  2.40s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:19<00:39,  3.05s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:22<00:37,  3.15s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:25<00:32,  2.98s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:28<00:29,  2.93s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:31<00:25,  2.84s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:32<00:19,  2.50s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:34<00:14,  2.12s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:35<00:12,  2.02s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:39<00:12,  2.42s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:42<00:10,  2.55s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:43<00:06,  2.24s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:46<00:04,  2.46s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:48<00:02,  2.38s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:49<00:00,  2.04s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:51<00:00,  2.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:51<00:00,  2.56s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 51.2235, 'train_samples_per_second': 6.247, 'train_steps_per_second': 0.39, 'train_loss': 0.6940070629119873, 'epoch': 1.0}
>> ==================== Round 2 : [1, 2] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:02<00:42,  2.23s/it] 10%|â–ˆ         | 2/20 [00:05<00:51,  2.88s/it] 15%|â–ˆâ–Œ        | 3/20 [00:07<00:42,  2.52s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:11<00:45,  2.85s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:12<00:37,  2.48s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:14<00:28,  2.05s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:15<00:23,  1.83s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:18<00:27,  2.25s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:19<00:21,  1.96s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:22<00:20,  2.05s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:23<00:16,  1.78s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:24<00:12,  1.55s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:25<00:10,  1.49s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:28<00:12,  2.00s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:31<00:10,  2.18s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:33<00:08,  2.05s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:34<00:05,  1.69s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:36<00:03,  1.79s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:37<00:01,  1.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:38<00:00,  1.56s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:41<00:00,  1.56s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:41<00:00,  2.08s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 41.6477, 'train_samples_per_second': 7.683, 'train_steps_per_second': 0.48, 'train_loss': 0.6127084255218506, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:03<01:03,  3.37s/it] 10%|â–ˆ         | 2/20 [00:05<00:49,  2.77s/it] 15%|â–ˆâ–Œ        | 3/20 [00:08<00:46,  2.74s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:10<00:41,  2.60s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:14<00:43,  2.88s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:15<00:31,  2.25s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:16<00:26,  2.01s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:19<00:28,  2.35s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:22<00:27,  2.50s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:26<00:29,  2.93s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:28<00:24,  2.67s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:31<00:21,  2.71s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:34<00:18,  2.69s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:35<00:14,  2.37s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:38<00:12,  2.48s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:41<00:11,  2.81s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:43<00:07,  2.50s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:45<00:04,  2.39s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:48<00:02,  2.48s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:50<00:00,  2.42s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:52<00:00,  2.42s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:52<00:00,  2.62s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 52.3463, 'train_samples_per_second': 6.113, 'train_steps_per_second': 0.382, 'train_loss': 0.6038981437683105, 'epoch': 1.0}
>> ==================== Round 3 : [0, 1] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:02<00:38,  2.03s/it] 10%|â–ˆ         | 2/20 [00:03<00:29,  1.62s/it] 15%|â–ˆâ–Œ        | 3/20 [00:08<00:53,  3.12s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:10<00:43,  2.70s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:11<00:32,  2.17s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:14<00:32,  2.31s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:16<00:28,  2.17s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:17<00:24,  2.07s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:19<00:20,  1.86s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:22<00:23,  2.32s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:26<00:24,  2.68s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:27<00:19,  2.42s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:29<00:15,  2.18s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:31<00:11,  1.96s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:32<00:08,  1.69s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:34<00:07,  1.77s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:34<00:04,  1.46s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:36<00:02,  1.40s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:37<00:01,  1.56s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:39<00:00,  1.67s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:43<00:00,  1.67s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:43<00:00,  2.17s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 43.4553, 'train_samples_per_second': 7.364, 'train_steps_per_second': 0.46, 'train_loss': 0.5779412746429443, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:27,  1.45s/it] 10%|â–ˆ         | 2/20 [00:02<00:21,  1.20s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:17,  1.03s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:04<00:15,  1.05it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:05<00:16,  1.08s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:06<00:12,  1.08it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:07<00:14,  1.10s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:09<00:17,  1.47s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:11<00:15,  1.45s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:14<00:20,  2.05s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:18<00:22,  2.47s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:19<00:17,  2.16s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:20<00:13,  1.87s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:22<00:10,  1.72s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:23<00:07,  1.59s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:23<00:05,  1.31s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:24<00:03,  1.13s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:27<00:03,  1.57s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:29<00:01,  1.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.84s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:39<00:00,  1.84s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:39<00:00,  1.98s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 39.5275, 'train_samples_per_second': 8.096, 'train_steps_per_second': 0.506, 'train_loss': 0.620793628692627, 'epoch': 1.0}
>> ==================== Round 4 : [3, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:34,  1.81s/it] 10%|â–ˆ         | 2/20 [00:03<00:36,  2.03s/it] 15%|â–ˆâ–Œ        | 3/20 [00:06<00:39,  2.33s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:07<00:29,  1.85s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:10<00:30,  2.03s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:11<00:24,  1.77s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:13<00:23,  1.80s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:14<00:19,  1.66s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:15<00:16,  1.48s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:17<00:16,  1.67s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:20<00:17,  1.97s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:22<00:16,  2.10s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:25<00:15,  2.21s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:27<00:13,  2.23s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:30<00:12,  2.48s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:32<00:09,  2.25s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:33<00:05,  1.95s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:35<00:03,  1.84s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:37<00:02,  2.08s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:39<00:00,  1.82s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:42<00:00,  1.82s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:42<00:00,  2.11s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 42.2105, 'train_samples_per_second': 7.581, 'train_steps_per_second': 0.474, 'train_loss': 0.5699963092803955, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:02<00:43,  2.27s/it] 10%|â–ˆ         | 2/20 [00:03<00:32,  1.79s/it] 15%|â–ˆâ–Œ        | 3/20 [00:06<00:34,  2.04s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:09<00:41,  2.58s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:12<00:42,  2.82s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:15<00:39,  2.82s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:17<00:32,  2.52s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:18<00:26,  2.21s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:20<00:20,  1.88s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:21<00:17,  1.72s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:22<00:13,  1.53s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:24<00:13,  1.68s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:27<00:13,  1.92s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:28<00:10,  1.70s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:30<00:09,  1.83s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:33<00:08,  2.07s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:34<00:05,  1.97s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:35<00:03,  1.73s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:37<00:01,  1.64s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:39<00:00,  1.88s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:41<00:00,  1.88s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:41<00:00,  2.06s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 41.2807, 'train_samples_per_second': 7.752, 'train_steps_per_second': 0.484, 'train_loss': 0.5738239288330078, 'epoch': 1.0}
>> ==================== Round 5 : [3, 4] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:02<00:52,  2.76s/it] 10%|â–ˆ         | 2/20 [00:04<00:41,  2.29s/it] 15%|â–ˆâ–Œ        | 3/20 [00:06<00:36,  2.14s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:07<00:25,  1.57s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:08<00:21,  1.42s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:09<00:16,  1.21s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:11<00:20,  1.55s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:14<00:22,  1.90s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:15<00:18,  1.69s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:19<00:23,  2.33s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:20<00:19,  2.13s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:23<00:17,  2.15s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:25<00:15,  2.19s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:27<00:13,  2.17s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:30<00:11,  2.30s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:31<00:08,  2.10s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:34<00:06,  2.28s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:37<00:04,  2.46s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:38<00:02,  2.09s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:40<00:00,  2.02s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:45<00:00,  2.02s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:45<00:00,  2.27s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 45.4787, 'train_samples_per_second': 7.036, 'train_steps_per_second': 0.44, 'train_loss': 0.5713196754455566, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:37,  1.99s/it] 10%|â–ˆ         | 2/20 [00:03<00:28,  1.59s/it] 15%|â–ˆâ–Œ        | 3/20 [00:05<00:33,  1.96s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:07<00:33,  2.09s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:10<00:34,  2.33s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:11<00:26,  1.87s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:12<00:21,  1.66s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:14<00:19,  1.66s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:17<00:21,  1.97s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:19<00:20,  2.09s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:21<00:16,  1.88s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:23<00:15,  1.94s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:25<00:13,  1.95s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:27<00:13,  2.20s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:30<00:11,  2.27s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:31<00:07,  1.87s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:32<00:05,  1.78s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:33<00:03,  1.59s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:35<00:01,  1.54s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:36<00:00,  1.40s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:42<00:00,  1.40s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:42<00:00,  2.15s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 42.9938, 'train_samples_per_second': 7.443, 'train_steps_per_second': 0.465, 'train_loss': 0.5553894996643066, 'epoch': 1.0}
>> ==================== Round 6 : [4, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:17,  1.06it/s] 10%|â–ˆ         | 2/20 [00:02<00:28,  1.56s/it] 15%|â–ˆâ–Œ        | 3/20 [00:05<00:32,  1.90s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:07<00:32,  2.04s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:09<00:30,  2.06s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:11<00:29,  2.12s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:14<00:31,  2.41s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:16<00:25,  2.11s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:17<00:20,  1.89s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:18<00:16,  1.64s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:20<00:14,  1.61s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:20<00:10,  1.32s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:22<00:09,  1.34s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:24<00:09,  1.56s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:26<00:09,  1.83s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:29<00:07,  1.95s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:30<00:05,  1.79s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:32<00:03,  1.71s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:34<00:01,  1.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:35<00:00,  1.72s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:36<00:00,  1.72s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:36<00:00,  1.85s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 36.9419, 'train_samples_per_second': 8.662, 'train_steps_per_second': 0.541, 'train_loss': 0.5707983493804931, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:03<00:58,  3.06s/it] 10%|â–ˆ         | 2/20 [00:04<00:37,  2.11s/it] 15%|â–ˆâ–Œ        | 3/20 [00:07<00:41,  2.42s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:09<00:36,  2.28s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:10<00:30,  2.02s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:12<00:28,  2.01s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:14<00:24,  1.85s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:16<00:22,  1.87s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:19<00:24,  2.24s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:21<00:22,  2.23s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:24<00:22,  2.54s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:27<00:20,  2.50s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:29<00:16,  2.39s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:30<00:11,  1.92s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:32<00:10,  2.12s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:34<00:08,  2.10s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:37<00:07,  2.33s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:43<00:06,  3.22s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:44<00:02,  2.66s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:47<00:00,  2.75s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:48<00:00,  2.75s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:48<00:00,  2.43s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 48.5972, 'train_samples_per_second': 6.585, 'train_steps_per_second': 0.412, 'train_loss': 0.5075170516967773, 'epoch': 1.0}
>> ==================== Round 7 : [1, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:03<01:03,  3.32s/it] 10%|â–ˆ         | 2/20 [00:06<01:03,  3.52s/it] 15%|â–ˆâ–Œ        | 3/20 [00:09<00:54,  3.20s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:13<00:53,  3.35s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:15<00:43,  2.89s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:16<00:33,  2.37s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:17<00:25,  1.92s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:19<00:20,  1.73s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:20<00:19,  1.75s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:23<00:18,  1.90s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:24<00:16,  1.82s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:27<00:16,  2.04s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:29<00:15,  2.21s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:31<00:12,  2.02s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:33<00:10,  2.03s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:35<00:07,  1.99s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:36<00:05,  1.72s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:38<00:03,  1.72s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:39<00:01,  1.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:41<00:00,  1.73s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:46<00:00,  1.73s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:46<00:00,  2.34s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 46.7287, 'train_samples_per_second': 6.848, 'train_steps_per_second': 0.428, 'train_loss': 0.5704239845275879, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:30,  1.61s/it] 10%|â–ˆ         | 2/20 [00:04<00:42,  2.34s/it] 15%|â–ˆâ–Œ        | 3/20 [00:05<00:32,  1.91s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:07<00:27,  1.71s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:10<00:36,  2.41s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:14<00:37,  2.71s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:15<00:29,  2.26s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:17<00:26,  2.22s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:20<00:26,  2.44s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:24<00:29,  2.95s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:26<00:22,  2.53s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:29<00:21,  2.69s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:32<00:19,  2.85s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:34<00:15,  2.57s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:36<00:11,  2.36s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:38<00:08,  2.18s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:39<00:06,  2.03s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:41<00:04,  2.08s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:44<00:02,  2.23s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:46<00:00,  2.20s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:48<00:00,  2.20s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:48<00:00,  2.41s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 48.2956, 'train_samples_per_second': 6.626, 'train_steps_per_second': 0.414, 'train_loss': 0.507079029083252, 'epoch': 1.0}
>> ==================== Round 8 : [2, 5] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:04<01:22,  4.33s/it] 10%|â–ˆ         | 2/20 [00:06<00:50,  2.81s/it] 15%|â–ˆâ–Œ        | 3/20 [00:09<00:54,  3.18s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:12<00:48,  3.01s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:15<00:43,  2.91s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:17<00:37,  2.67s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:18<00:30,  2.32s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:20<00:26,  2.21s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:23<00:24,  2.19s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:26<00:24,  2.46s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:29<00:24,  2.78s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:33<00:23,  2.98s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:36<00:21,  3.11s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:39<00:19,  3.21s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:41<00:13,  2.67s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:43<00:10,  2.57s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:44<00:06,  2.16s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:50<00:06,  3.17s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:54<00:03,  3.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:58<00:00,  3.70s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:04<00:00,  3.70s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:04<00:00,  3.23s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 64.6879, 'train_samples_per_second': 4.947, 'train_steps_per_second': 0.309, 'train_loss': 0.5308649063110351, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:03<01:04,  3.42s/it] 10%|â–ˆ         | 2/20 [00:06<01:01,  3.42s/it] 15%|â–ˆâ–Œ        | 3/20 [00:08<00:47,  2.80s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:12<00:47,  2.96s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:14<00:43,  2.88s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:17<00:39,  2.80s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:19<00:31,  2.42s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:20<00:23,  1.99s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:21<00:19,  1.77s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:24<00:20,  2.03s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:27<00:22,  2.50s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:30<00:20,  2.59s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:35<00:22,  3.20s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:40<00:23,  3.93s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:42<00:16,  3.25s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:43<00:10,  2.73s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:46<00:07,  2.56s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:48<00:04,  2.49s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:51<00:02,  2.69s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:53<00:00,  2.54s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:55<00:00,  2.54s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:55<00:00,  2.76s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 55.2349, 'train_samples_per_second': 5.793, 'train_steps_per_second': 0.362, 'train_loss': 0.5372026920318603, 'epoch': 1.0}
>> ==================== Round 9 : [3, 5] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:03<01:09,  3.68s/it] 10%|â–ˆ         | 2/20 [00:05<00:48,  2.71s/it] 15%|â–ˆâ–Œ        | 3/20 [00:07<00:35,  2.07s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:08<00:27,  1.72s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:10<00:27,  1.83s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:12<00:28,  2.05s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:15<00:29,  2.24s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:21<00:41,  3.42s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:23<00:34,  3.11s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:26<00:31,  3.14s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:29<00:25,  2.82s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:30<00:19,  2.49s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:31<00:14,  2.10s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:33<00:11,  1.98s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:34<00:08,  1.62s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:36<00:06,  1.60s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:36<00:03,  1.32s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:38<00:02,  1.37s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:40<00:01,  1.73s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:43<00:00,  2.06s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:45<00:00,  2.06s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:45<00:00,  2.26s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 45.1654, 'train_samples_per_second': 7.085, 'train_steps_per_second': 0.443, 'train_loss': 0.5078556060791015, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:22,  1.19s/it] 10%|â–ˆ         | 2/20 [00:02<00:19,  1.06s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:20,  1.19s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:04<00:18,  1.17s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:19,  1.30s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:20,  1.48s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:12<00:31,  2.43s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:13<00:25,  2.13s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:16<00:24,  2.22s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:18<00:20,  2.10s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:20<00:19,  2.12s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:22<00:16,  2.05s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:23<00:13,  1.96s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:25<00:11,  1.91s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:27<00:08,  1.73s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:29<00:08,  2.09s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:33<00:07,  2.46s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:35<00:04,  2.34s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:39<00:02,  2.76s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:40<00:00,  2.22s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:41<00:00,  2.22s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:41<00:00,  2.07s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 41.4796, 'train_samples_per_second': 7.715, 'train_steps_per_second': 0.482, 'train_loss': 0.5147034645080566, 'epoch': 1.0}
>> ==================== Round 10 : [5, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:31,  1.68s/it] 10%|â–ˆ         | 2/20 [00:06<01:00,  3.34s/it] 15%|â–ˆâ–Œ        | 3/20 [00:09<00:57,  3.38s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:12<00:48,  3.06s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:14<00:42,  2.81s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:15<00:32,  2.29s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:17<00:26,  2.05s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:19<00:23,  1.93s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:23<00:29,  2.69s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:26<00:27,  2.78s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:29<00:26,  2.94s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:31<00:21,  2.64s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:33<00:17,  2.47s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:35<00:13,  2.21s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:37<00:10,  2.12s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:40<00:09,  2.46s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:42<00:06,  2.31s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:46<00:05,  2.70s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:49<00:03,  3.05s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:51<00:00,  2.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:52<00:00,  2.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:52<00:00,  2.62s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 52.3606, 'train_samples_per_second': 6.111, 'train_steps_per_second': 0.382, 'train_loss': 0.4781501293182373, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:04<01:28,  4.68s/it] 10%|â–ˆ         | 2/20 [00:06<00:50,  2.82s/it] 15%|â–ˆâ–Œ        | 3/20 [00:09<00:49,  2.93s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:11<00:42,  2.67s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:14<00:41,  2.78s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:15<00:29,  2.14s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:17<00:26,  2.07s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:19<00:24,  2.04s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:21<00:23,  2.11s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:24<00:23,  2.38s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:26<00:19,  2.20s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:27<00:16,  2.02s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:30<00:14,  2.06s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:31<00:10,  1.77s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:33<00:09,  1.92s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:34<00:06,  1.63s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:35<00:04,  1.50s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:37<00:02,  1.49s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:38<00:01,  1.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:42<00:00,  2.27s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:44<00:00,  2.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:44<00:00,  2.20s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 44.1006, 'train_samples_per_second': 7.256, 'train_steps_per_second': 0.454, 'train_loss': 0.5119056224822998, 'epoch': 1.0}
>> ==================== Round 11 : [0, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:18,  1.02it/s] 10%|â–ˆ         | 2/20 [00:02<00:24,  1.36s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:26,  1.57s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:06<00:26,  1.69s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:07<00:24,  1.61s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:09<00:25,  1.81s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:11<00:22,  1.76s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:13<00:22,  1.89s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:16<00:24,  2.20s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:18<00:20,  2.03s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:19<00:15,  1.78s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:20<00:13,  1.65s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:21<00:10,  1.44s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:23<00:09,  1.59s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:25<00:08,  1.63s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:27<00:06,  1.71s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:28<00:04,  1.57s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:32<00:04,  2.36s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:35<00:02,  2.43s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:37<00:00,  2.28s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:41<00:00,  2.28s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:41<00:00,  2.05s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 41.0779, 'train_samples_per_second': 7.79, 'train_steps_per_second': 0.487, 'train_loss': 0.47946391105651853, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:23,  1.22s/it] 10%|â–ˆ         | 2/20 [00:03<00:32,  1.81s/it] 15%|â–ˆâ–Œ        | 3/20 [00:06<00:42,  2.52s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:09<00:42,  2.64s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:12<00:39,  2.65s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:14<00:35,  2.53s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:16<00:30,  2.34s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:18<00:25,  2.16s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:22<00:30,  2.76s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:25<00:27,  2.77s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:28<00:26,  2.91s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:30<00:21,  2.64s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:31<00:15,  2.26s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:34<00:13,  2.33s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:35<00:09,  1.86s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:36<00:07,  1.78s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:38<00:05,  1.69s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:40<00:03,  1.91s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:41<00:01,  1.76s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:44<00:00,  2.13s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:46<00:00,  2.13s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:46<00:00,  2.34s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 46.8328, 'train_samples_per_second': 6.833, 'train_steps_per_second': 0.427, 'train_loss': 0.48471627235412595, 'epoch': 1.0}
>> ==================== Round 12 : [7, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:32,  1.73s/it] 10%|â–ˆ         | 2/20 [00:02<00:26,  1.45s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:19,  1.12s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:21,  1.36s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:07<00:21,  1.46s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:09<00:24,  1.72s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:11<00:24,  1.90s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:13<00:23,  1.96s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:15<00:21,  1.91s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:18<00:22,  2.29s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:20<00:18,  2.04s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:21<00:15,  1.98s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:23<00:12,  1.85s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:24<00:09,  1.64s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:25<00:07,  1.47s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:27<00:05,  1.49s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:28<00:04,  1.42s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:30<00:02,  1.45s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:33<00:02,  2.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:35<00:00,  2.12s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:43<00:00,  2.12s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:43<00:00,  2.17s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 43.3515, 'train_samples_per_second': 7.382, 'train_steps_per_second': 0.461, 'train_loss': 0.5331921577453613, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:02<00:40,  2.15s/it] 10%|â–ˆ         | 2/20 [00:05<00:51,  2.84s/it] 15%|â–ˆâ–Œ        | 3/20 [00:07<00:44,  2.60s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:08<00:31,  1.98s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:10<00:30,  2.03s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:13<00:29,  2.12s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:14<00:23,  1.79s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:15<00:20,  1.68s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:17<00:19,  1.77s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:20<00:19,  1.95s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:22<00:18,  2.03s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:24<00:17,  2.19s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:27<00:16,  2.32s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:37<00:27,  4.54s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:38<00:17,  3.47s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:39<00:11,  2.82s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:41<00:07,  2.59s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:43<00:04,  2.39s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:44<00:02,  2.14s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:46<00:00,  1.96s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:47<00:00,  1.96s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:47<00:00,  2.39s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 47.7294, 'train_samples_per_second': 6.704, 'train_steps_per_second': 0.419, 'train_loss': 0.5083989143371582, 'epoch': 1.0}
>> ==================== Round 13 : [4, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:27,  1.44s/it] 10%|â–ˆ         | 2/20 [00:02<00:26,  1.45s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:25,  1.48s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:21,  1.37s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:17,  1.14s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:07<00:14,  1.06s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:08<00:15,  1.19s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:09<00:13,  1.16s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:11<00:13,  1.27s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:12<00:11,  1.13s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:13<00:10,  1.14s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:15<00:11,  1.39s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:16<00:09,  1.41s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:17<00:08,  1.36s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:19<00:06,  1.34s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:20<00:05,  1.31s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:22<00:04,  1.45s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:23<00:02,  1.47s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:24<00:01,  1.33s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:26<00:00,  1.49s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.49s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.40s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 28.0339, 'train_samples_per_second': 11.415, 'train_steps_per_second': 0.713, 'train_loss': 0.5696717262268066, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.27it/s] 10%|â–ˆ         | 2/20 [00:01<00:15,  1.13it/s] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:22,  1.31s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:04<00:19,  1.24s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:05<00:18,  1.24s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:06<00:14,  1.04s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:08<00:16,  1.24s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:10<00:18,  1.50s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:11<00:15,  1.45s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:13<00:16,  1.66s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:15<00:13,  1.53s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:15<00:10,  1.33s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:17<00:10,  1.43s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:18<00:08,  1.40s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:19<00:06,  1.27s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:21<00:05,  1.33s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:22<00:03,  1.20s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:23<00:02,  1.08s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:24<00:01,  1.08s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:25<00:00,  1.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.24s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.37s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 27.3766, 'train_samples_per_second': 11.689, 'train_steps_per_second': 0.731, 'train_loss': 0.5033932209014893, 'epoch': 1.0}
>> ==================== Round 14 : [4, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.19it/s] 10%|â–ˆ         | 2/20 [00:01<00:13,  1.30it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:14,  1.15it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:04<00:20,  1.30s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:24,  1.63s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:07<00:20,  1.47s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:08<00:17,  1.31s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:10<00:17,  1.43s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:12<00:16,  1.54s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:13<00:15,  1.53s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:14<00:12,  1.37s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:16<00:11,  1.46s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:18<00:10,  1.48s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:19<00:09,  1.60s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:20<00:06,  1.39s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:22<00:06,  1.59s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:24<00:04,  1.66s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:25<00:02,  1.50s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:26<00:01,  1.30s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.12s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.12s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.58s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 31.5443, 'train_samples_per_second': 10.144, 'train_steps_per_second': 0.634, 'train_loss': 0.4891488552093506, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:30,  1.63s/it] 10%|â–ˆ         | 2/20 [00:02<00:22,  1.27s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:19,  1.14s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:22,  1.40s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:21,  1.42s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:09<00:23,  1.68s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:10<00:18,  1.45s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:18,  1.51s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:17,  1.55s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:14<00:15,  1.52s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:16<00:14,  1.58s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:18<00:12,  1.58s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:22<00:17,  2.48s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:24<00:12,  2.15s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:25<00:09,  1.88s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:26<00:06,  1.74s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:29<00:05,  1.93s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:29<00:03,  1.63s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:31<00:01,  1.45s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.30s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:34<00:00,  1.30s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:34<00:00,  1.72s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 34.381, 'train_samples_per_second': 9.307, 'train_steps_per_second': 0.582, 'train_loss': 0.47199525833129885, 'epoch': 1.0}
>> ==================== Round 15 : [1, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:16,  1.13it/s] 10%|â–ˆ         | 2/20 [00:03<00:34,  1.93s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:28,  1.70s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:06<00:23,  1.48s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:07<00:20,  1.38s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:07<00:15,  1.13s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:09<00:15,  1.20s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:17,  1.47s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:16,  1.54s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:14<00:14,  1.44s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:15<00:13,  1.47s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:17<00:12,  1.57s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:19<00:10,  1.56s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:20<00:09,  1.54s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:22<00:07,  1.53s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:23<00:05,  1.42s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:25<00:04,  1.52s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:26<00:02,  1.40s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:28<00:01,  1.69s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:35<00:00,  1.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:35<00:00,  1.76s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 35.1056, 'train_samples_per_second': 9.115, 'train_steps_per_second': 0.57, 'train_loss': 0.49398250579833985, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:29,  1.58s/it] 10%|â–ˆ         | 2/20 [00:02<00:22,  1.24s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:19,  1.14s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:04<00:18,  1.16s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:05<00:15,  1.06s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:07<00:16,  1.18s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:08<00:16,  1.23s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:20,  1.68s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:12<00:16,  1.48s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:14<00:16,  1.61s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:15<00:13,  1.53s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:16<00:10,  1.31s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:17<00:08,  1.24s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:19<00:09,  1.51s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:21<00:07,  1.55s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:22<00:05,  1.40s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:23<00:04,  1.38s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:24<00:02,  1.42s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:26<00:01,  1.47s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.37s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.37s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.45s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 28.9108, 'train_samples_per_second': 11.069, 'train_steps_per_second': 0.692, 'train_loss': 0.48728036880493164, 'epoch': 1.0}
>> ==================== Round 16 : [0, 3] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:18,  1.00it/s] 10%|â–ˆ         | 2/20 [00:02<00:21,  1.22s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:23,  1.41s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:04<00:18,  1.18s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:05<00:16,  1.07s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:07<00:16,  1.16s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:08<00:18,  1.42s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:10<00:16,  1.36s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:11<00:14,  1.33s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:12<00:13,  1.31s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:13<00:11,  1.29s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:15<00:09,  1.24s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:16<00:08,  1.26s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:17<00:06,  1.11s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:18<00:05,  1.19s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:19<00:05,  1.26s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:21<00:04,  1.38s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:23<00:03,  1.54s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:24<00:01,  1.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:25<00:00,  1.38s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:26<00:00,  1.38s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:26<00:00,  1.35s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 26.9686, 'train_samples_per_second': 11.866, 'train_steps_per_second': 0.742, 'train_loss': 0.5197914123535157, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:26,  1.40s/it] 10%|â–ˆ         | 2/20 [00:02<00:25,  1.42s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:26,  1.58s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:06<00:26,  1.63s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:07<00:20,  1.35s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:07<00:15,  1.14s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:08<00:14,  1.08s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:10<00:15,  1.25s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:11<00:14,  1.27s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:13<00:13,  1.39s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:14<00:11,  1.24s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:15<00:08,  1.10s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:16<00:07,  1.03s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:16<00:05,  1.09it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:18<00:06,  1.25s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:19<00:04,  1.12s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:20<00:03,  1.10s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:21<00:02,  1.15s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:22<00:01,  1.06s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:24<00:00,  1.28s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:26<00:00,  1.28s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:26<00:00,  1.30s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 26.0953, 'train_samples_per_second': 12.263, 'train_steps_per_second': 0.766, 'train_loss': 0.49804201126098635, 'epoch': 1.0}
>> ==================== Round 17 : [5, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:02<00:47,  2.49s/it] 10%|â–ˆ         | 2/20 [00:03<00:31,  1.73s/it] 15%|â–ˆâ–Œ        | 3/20 [00:05<00:30,  1.82s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:06<00:21,  1.36s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:07<00:21,  1.41s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:09<00:22,  1.60s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:11<00:20,  1.60s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:12<00:15,  1.33s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:14,  1.35s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:15<00:14,  1.41s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:16<00:12,  1.39s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:18<00:12,  1.53s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:19<00:09,  1.42s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:20<00:08,  1.36s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:23<00:08,  1.70s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:24<00:06,  1.65s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:26<00:04,  1.57s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:26<00:02,  1.31s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:28<00:01,  1.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:30<00:00,  1.66s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.66s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.63s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 32.6292, 'train_samples_per_second': 9.807, 'train_steps_per_second': 0.613, 'train_loss': 0.4614450454711914, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:26,  1.39s/it] 10%|â–ˆ         | 2/20 [00:02<00:24,  1.37s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:17,  1.06s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:22,  1.39s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:07<00:25,  1.72s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:19,  1.43s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:10<00:21,  1.67s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:18,  1.55s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:17,  1.55s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:14<00:14,  1.49s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:16<00:13,  1.49s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:17<00:11,  1.50s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:19<00:09,  1.43s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:20<00:09,  1.51s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:21<00:06,  1.34s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:23<00:05,  1.34s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:24<00:04,  1.42s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:26<00:02,  1.39s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:26<00:01,  1.14s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.32s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.32s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.48s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 29.5206, 'train_samples_per_second': 10.84, 'train_steps_per_second': 0.677, 'train_loss': 0.49956583976745605, 'epoch': 1.0}
>> ==================== Round 18 : [6, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:33,  1.78s/it] 10%|â–ˆ         | 2/20 [00:03<00:32,  1.78s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:21,  1.29s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:17,  1.12s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:19,  1.28s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:07<00:17,  1.25s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:08<00:15,  1.20s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:09<00:13,  1.11s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:11<00:15,  1.41s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:12<00:12,  1.25s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:14<00:12,  1.38s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:15<00:11,  1.40s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:17<00:10,  1.52s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:18<00:07,  1.33s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:20<00:06,  1.33s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:22<00:06,  1.58s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:23<00:04,  1.55s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:25<00:02,  1.50s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:25<00:01,  1.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:26<00:00,  1.07s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.61s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 32.3024, 'train_samples_per_second': 9.906, 'train_steps_per_second': 0.619, 'train_loss': 0.4695390224456787, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:02<00:41,  2.21s/it] 10%|â–ˆ         | 2/20 [00:03<00:30,  1.71s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:25,  1.51s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:21,  1.36s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:07<00:22,  1.47s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:19,  1.39s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:09<00:16,  1.30s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:10<00:13,  1.13s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:11<00:12,  1.09s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:12<00:10,  1.09s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:14<00:10,  1.18s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:15<00:09,  1.13s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:16<00:09,  1.30s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:18<00:07,  1.28s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:19<00:06,  1.28s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:20<00:04,  1.16s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:22<00:04,  1.42s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:24<00:03,  1.54s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:25<00:01,  1.47s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:26<00:00,  1.28s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.28s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.42s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 28.3837, 'train_samples_per_second': 11.274, 'train_steps_per_second': 0.705, 'train_loss': 0.508644962310791, 'epoch': 1.0}
>> ==================== Round 19 : [1, 2] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:24,  1.31s/it] 10%|â–ˆ         | 2/20 [00:02<00:22,  1.26s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:25,  1.48s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:21,  1.35s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:07<00:21,  1.45s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:21,  1.52s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:10<00:20,  1.59s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:18,  1.55s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:15,  1.41s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:13<00:12,  1.28s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:15<00:11,  1.33s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:16<00:09,  1.23s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:17<00:08,  1.15s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:18<00:06,  1.07s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:20<00:07,  1.43s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:21<00:05,  1.38s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:23<00:04,  1.41s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:24<00:02,  1.35s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:25<00:01,  1.20s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:26<00:00,  1.22s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.22s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.41s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 28.3007, 'train_samples_per_second': 11.307, 'train_steps_per_second': 0.707, 'train_loss': 0.5287514209747315, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:30,  1.62s/it] 10%|â–ˆ         | 2/20 [00:03<00:34,  1.94s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:26,  1.57s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:21,  1.32s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:18,  1.23s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:18,  1.30s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:09<00:16,  1.26s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:10<00:14,  1.21s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:17,  1.64s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:14<00:14,  1.45s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:14<00:10,  1.20s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:16<00:11,  1.41s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:19<00:13,  1.87s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:21<00:11,  1.92s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:22<00:07,  1.56s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:23<00:06,  1.55s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:24<00:03,  1.30s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:26<00:02,  1.35s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:28<00:01,  1.71s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:30<00:00,  1.77s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.77s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.63s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 32.6324, 'train_samples_per_second': 9.806, 'train_steps_per_second': 0.613, 'train_loss': 0.530967903137207, 'epoch': 1.0}
>> ==================== Round 20 : [0, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:34,  1.80s/it] 10%|â–ˆ         | 2/20 [00:03<00:33,  1.86s/it] 15%|â–ˆâ–Œ        | 3/20 [00:05<00:30,  1.78s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:06<00:24,  1.52s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:07<00:20,  1.34s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:17,  1.24s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:09<00:14,  1.11s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:16,  1.38s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:16,  1.50s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:14<00:14,  1.48s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:16<00:13,  1.47s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:17<00:10,  1.36s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:18<00:09,  1.40s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:19<00:07,  1.22s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:20<00:05,  1.19s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:21<00:04,  1.10s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:22<00:03,  1.17s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:23<00:02,  1.05s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:25<00:01,  1.18s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.87s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.49s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 29.8789, 'train_samples_per_second': 10.71, 'train_steps_per_second': 0.669, 'train_loss': 0.4687164306640625, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:35,  1.87s/it] 10%|â–ˆ         | 2/20 [00:03<00:25,  1.43s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:22,  1.30s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:20,  1.31s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:20,  1.39s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:22,  1.59s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:09<00:17,  1.34s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:10<00:15,  1.27s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:12<00:15,  1.37s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:13<00:12,  1.24s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:15<00:12,  1.37s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:16<00:10,  1.31s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:17<00:09,  1.32s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:19<00:08,  1.35s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:20<00:06,  1.25s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:22<00:06,  1.54s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:23<00:04,  1.40s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:24<00:02,  1.42s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:26<00:01,  1.35s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.37s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:30<00:00,  1.37s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:30<00:00,  1.53s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 30.6933, 'train_samples_per_second': 10.426, 'train_steps_per_second': 0.652, 'train_loss': 0.4755120277404785, 'epoch': 1.0}
>> ==================== Round 21 : [2, 4] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:25,  1.33s/it] 10%|â–ˆ         | 2/20 [00:03<00:30,  1.70s/it] 15%|â–ˆâ–Œ        | 3/20 [00:05<00:31,  1.83s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:06<00:28,  1.77s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:09<00:32,  2.14s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:11<00:28,  2.04s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:12<00:23,  1.80s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:14<00:20,  1.69s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:15<00:18,  1.65s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:17<00:17,  1.73s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:18<00:13,  1.49s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:19<00:10,  1.31s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:21<00:10,  1.55s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:22<00:08,  1.42s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:25<00:09,  1.88s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:27<00:07,  1.91s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:29<00:05,  1.70s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:30<00:03,  1.55s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:31<00:01,  1.47s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:33<00:00,  1.48s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:37<00:00,  1.48s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:37<00:00,  1.86s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 37.2036, 'train_samples_per_second': 8.601, 'train_steps_per_second': 0.538, 'train_loss': 0.48466129302978517, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:18,  1.04it/s] 10%|â–ˆ         | 2/20 [00:01<00:17,  1.02it/s] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:18,  1.08s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:04<00:18,  1.18s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:21,  1.46s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:07<00:20,  1.45s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:09<00:17,  1.38s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:10<00:15,  1.30s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:10<00:11,  1.08s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:11<00:10,  1.05s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:13<00:11,  1.27s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:15<00:10,  1.35s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:16<00:09,  1.29s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:19<00:10,  1.79s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:20<00:07,  1.56s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:22<00:06,  1.66s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:22<00:04,  1.42s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:23<00:02,  1.27s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:25<00:01,  1.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:26<00:00,  1.30s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.30s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.41s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 28.2644, 'train_samples_per_second': 11.322, 'train_steps_per_second': 0.708, 'train_loss': 0.531153392791748, 'epoch': 1.0}
>> ==================== Round 22 : [2, 6] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:19,  1.02s/it] 10%|â–ˆ         | 2/20 [00:02<00:20,  1.14s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:18,  1.11s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:23,  1.44s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:22,  1.53s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:22,  1.61s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:09<00:17,  1.35s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:18,  1.50s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:12<00:14,  1.34s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:13<00:14,  1.41s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:15<00:14,  1.59s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:17<00:12,  1.53s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:18<00:10,  1.56s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:19<00:08,  1.41s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:21<00:06,  1.30s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:22<00:05,  1.38s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:24<00:04,  1.60s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:27<00:03,  1.87s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:28<00:01,  1.75s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:30<00:00,  1.63s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.63s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.57s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 31.3726, 'train_samples_per_second': 10.2, 'train_steps_per_second': 0.637, 'train_loss': 0.5030838012695312, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.46it/s] 10%|â–ˆ         | 2/20 [00:01<00:15,  1.18it/s] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:16,  1.01it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:23,  1.47s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:21,  1.44s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:07<00:16,  1.16s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:08<00:16,  1.26s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:09<00:14,  1.20s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:10<00:11,  1.08s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:11<00:10,  1.05s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:12<00:09,  1.00s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:13<00:09,  1.22s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:16<00:10,  1.49s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:17<00:09,  1.62s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:19<00:07,  1.47s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:21<00:06,  1.65s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:22<00:04,  1.40s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:23<00:02,  1.39s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:25<00:01,  1.48s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:26<00:00,  1.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.37s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 27.5026, 'train_samples_per_second': 11.635, 'train_steps_per_second': 0.727, 'train_loss': 0.49558405876159667, 'epoch': 1.0}
>> ==================== Round 23 : [2, 3] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:35,  1.88s/it] 10%|â–ˆ         | 2/20 [00:03<00:27,  1.55s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:20,  1.20s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:20,  1.25s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:07<00:21,  1.42s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:09<00:22,  1.64s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:10<00:19,  1.52s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:17,  1.48s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:18,  1.69s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:15<00:17,  1.75s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:17<00:14,  1.63s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:18<00:11,  1.44s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:20<00:11,  1.62s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:21<00:08,  1.38s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:22<00:06,  1.33s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:23<00:04,  1.17s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:24<00:03,  1.21s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:26<00:02,  1.44s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:28<00:01,  1.53s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.58s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.58s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.63s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 32.5942, 'train_samples_per_second': 9.818, 'train_steps_per_second': 0.614, 'train_loss': 0.5035819053649903, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:26,  1.37s/it] 10%|â–ˆ         | 2/20 [00:02<00:19,  1.07s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:25,  1.51s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:20,  1.31s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:17,  1.13s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:20,  1.43s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:09<00:18,  1.43s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:17,  1.47s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:12<00:14,  1.34s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:13<00:12,  1.30s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:15<00:12,  1.41s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:16<00:10,  1.32s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:17<00:08,  1.27s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:18<00:07,  1.23s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:19<00:05,  1.20s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:20<00:04,  1.22s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:22<00:03,  1.26s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:23<00:02,  1.15s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:24<00:01,  1.21s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:25<00:00,  1.08s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.08s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.44s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 28.7619, 'train_samples_per_second': 11.126, 'train_steps_per_second': 0.695, 'train_loss': 0.4815244674682617, 'epoch': 1.0}
>> ==================== Round 24 : [1, 4] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:33,  1.75s/it] 10%|â–ˆ         | 2/20 [00:02<00:23,  1.29s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:20,  1.18s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:06<00:27,  1.72s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:08<00:25,  1.72s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:18,  1.35s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:10<00:18,  1.42s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:16,  1.34s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:12<00:13,  1.27s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:13<00:13,  1.30s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:15<00:12,  1.41s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:16<00:11,  1.38s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:18<00:10,  1.46s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:19<00:08,  1.36s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:20<00:06,  1.35s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:21<00:04,  1.14s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:23<00:03,  1.22s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:23<00:02,  1.06s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:25<00:01,  1.30s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.65s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.46s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 29.2518, 'train_samples_per_second': 10.939, 'train_steps_per_second': 0.684, 'train_loss': 0.4819972038269043, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:24,  1.26s/it] 10%|â–ˆ         | 2/20 [00:03<00:33,  1.87s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:25,  1.48s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:21,  1.35s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:07<00:23,  1.57s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:18,  1.31s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:10<00:18,  1.39s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:16,  1.35s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:12<00:13,  1.26s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:13<00:12,  1.28s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:14<00:10,  1.11s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:15<00:09,  1.21s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:17<00:09,  1.35s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:19<00:08,  1.41s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:20<00:06,  1.40s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:21<00:05,  1.29s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:23<00:04,  1.47s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:24<00:02,  1.22s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:25<00:01,  1.33s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.52s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:30<00:00,  1.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:30<00:00,  1.51s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 30.2637, 'train_samples_per_second': 10.574, 'train_steps_per_second': 0.661, 'train_loss': 0.48131756782531737, 'epoch': 1.0}
>> ==================== Round 25 : [2, 6] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:16,  1.14it/s] 10%|â–ˆ         | 2/20 [00:02<00:26,  1.46s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:24,  1.46s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:24,  1.51s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:20,  1.34s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:21,  1.54s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:10<00:20,  1.56s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:12<00:19,  1.65s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:18,  1.65s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:15<00:15,  1.51s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:16<00:14,  1.56s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:18<00:12,  1.57s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:19<00:10,  1.52s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:21<00:09,  1.54s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:22<00:07,  1.51s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:25<00:07,  1.80s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:26<00:04,  1.63s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:27<00:02,  1.48s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:28<00:01,  1.33s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:30<00:00,  1.37s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.37s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.56s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 31.2819, 'train_samples_per_second': 10.23, 'train_steps_per_second': 0.639, 'train_loss': 0.47378015518188477, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:17,  1.11it/s] 10%|â–ˆ         | 2/20 [00:02<00:28,  1.56s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:22,  1.31s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:21,  1.35s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:17,  1.19s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:20,  1.48s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:09<00:18,  1.43s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:12<00:21,  1.76s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:16,  1.52s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:15<00:18,  1.86s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:17<00:15,  1.70s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:18<00:13,  1.72s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:20<00:11,  1.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:21<00:09,  1.63s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:22<00:07,  1.49s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:24<00:06,  1.50s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:25<00:03,  1.31s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:27<00:02,  1.41s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:28<00:01,  1.46s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.56s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 31.2968, 'train_samples_per_second': 10.225, 'train_steps_per_second': 0.639, 'train_loss': 0.4819822311401367, 'epoch': 1.0}
>> ==================== Round 26 : [0, 6] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:21,  1.11s/it] 10%|â–ˆ         | 2/20 [00:03<00:35,  1.95s/it] 15%|â–ˆâ–Œ        | 3/20 [00:05<00:32,  1.92s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:07<00:30,  1.89s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:08<00:26,  1.76s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:09<00:21,  1.51s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:11<00:19,  1.50s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:13<00:20,  1.70s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:14<00:17,  1.57s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:16<00:14,  1.46s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:17<00:13,  1.50s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:19<00:11,  1.49s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:21<00:11,  1.66s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:22<00:08,  1.46s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:24<00:08,  1.62s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:26<00:07,  1.76s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:28<00:05,  1.96s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:29<00:03,  1.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:31<00:01,  1.61s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:39<00:00,  1.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:39<00:00,  1.96s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 39.2803, 'train_samples_per_second': 8.147, 'train_steps_per_second': 0.509, 'train_loss': 0.47655363082885743, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:35,  1.87s/it] 10%|â–ˆ         | 2/20 [00:03<00:27,  1.50s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:23,  1.36s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:19,  1.24s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:19,  1.30s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:18,  1.35s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:10<00:23,  1.78s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:12<00:22,  1.84s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:15<00:22,  2.07s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:17<00:20,  2.06s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:18<00:16,  1.81s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:19<00:11,  1.50s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:21<00:10,  1.55s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:22<00:09,  1.54s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:24<00:07,  1.49s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:25<00:05,  1.39s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:26<00:04,  1.48s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:28<00:03,  1.54s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:29<00:01,  1.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:30<00:00,  1.38s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:35<00:00,  1.38s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:35<00:00,  1.78s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 35.5108, 'train_samples_per_second': 9.011, 'train_steps_per_second': 0.563, 'train_loss': 0.47774786949157716, 'epoch': 1.0}
>> ==================== Round 27 : [3, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:02<00:38,  2.01s/it] 10%|â–ˆ         | 2/20 [00:03<00:28,  1.57s/it] 15%|â–ˆâ–Œ        | 3/20 [00:05<00:34,  2.00s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:06<00:25,  1.59s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:08<00:23,  1.58s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:09<00:22,  1.59s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:10<00:17,  1.31s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:12<00:16,  1.38s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:14,  1.28s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:13<00:11,  1.10s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:15<00:10,  1.13s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:17<00:11,  1.43s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:18<00:10,  1.48s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:20<00:08,  1.45s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:23<00:09,  1.90s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:25<00:07,  1.92s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:26<00:05,  1.71s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:27<00:03,  1.64s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:29<00:01,  1.64s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:30<00:00,  1.48s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.48s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:31<00:00,  1.59s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 31.8752, 'train_samples_per_second': 10.039, 'train_steps_per_second': 0.627, 'train_loss': 0.4956183910369873, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:27,  1.43s/it] 10%|â–ˆ         | 2/20 [00:03<00:30,  1.71s/it] 15%|â–ˆâ–Œ        | 3/20 [00:05<00:29,  1.74s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:06<00:22,  1.44s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:07<00:21,  1.44s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:09<00:21,  1.52s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:10<00:17,  1.33s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:15,  1.33s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:12<00:14,  1.29s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:14<00:13,  1.34s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:16<00:14,  1.62s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:18<00:13,  1.64s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:18<00:09,  1.40s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:20<00:08,  1.40s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:21<00:06,  1.28s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:23<00:05,  1.43s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:23<00:03,  1.27s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:25<00:02,  1.30s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:26<00:01,  1.25s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.24s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.24s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.46s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 29.1882, 'train_samples_per_second': 10.963, 'train_steps_per_second': 0.685, 'train_loss': 0.4791556835174561, 'epoch': 1.0}
>> ==================== Round 28 : [4, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:34,  1.83s/it] 10%|â–ˆ         | 2/20 [00:02<00:25,  1.40s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:28,  1.66s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:06<00:27,  1.70s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:08<00:23,  1.59s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:09<00:19,  1.39s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:12<00:25,  1.93s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:12<00:18,  1.55s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:14<00:17,  1.62s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:15<00:14,  1.43s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:17<00:13,  1.48s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:18<00:11,  1.47s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:19<00:09,  1.41s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:21<00:08,  1.41s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:22<00:06,  1.35s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:23<00:05,  1.38s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:25<00:04,  1.49s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:27<00:03,  1.56s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:28<00:01,  1.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.39s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.39s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.60s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 32.0919, 'train_samples_per_second': 9.971, 'train_steps_per_second': 0.623, 'train_loss': 0.4906186103820801, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:21,  1.12s/it] 10%|â–ˆ         | 2/20 [00:02<00:23,  1.29s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:23,  1.38s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:23,  1.49s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:20,  1.37s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:19,  1.40s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:10<00:20,  1.61s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:17,  1.47s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:13<00:16,  1.50s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:14<00:14,  1.49s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:16<00:13,  1.54s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:16<00:10,  1.32s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:18<00:09,  1.40s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:19<00:07,  1.23s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:20<00:06,  1.29s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:21<00:04,  1.15s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:23<00:03,  1.32s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:25<00:02,  1.47s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:26<00:01,  1.45s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.41s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.41s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:29<00:00,  1.46s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 29.1454, 'train_samples_per_second': 10.979, 'train_steps_per_second': 0.686, 'train_loss': 0.48546342849731444, 'epoch': 1.0}
>> ==================== Round 29 : [1, 2] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:23,  1.23s/it] 10%|â–ˆ         | 2/20 [00:02<00:18,  1.02s/it] 15%|â–ˆâ–Œ        | 3/20 [00:02<00:14,  1.15it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:03<00:15,  1.06it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:05<00:17,  1.14s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:06<00:16,  1.17s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:07<00:15,  1.19s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:08<00:13,  1.16s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:10<00:15,  1.38s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:11<00:13,  1.33s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:12<00:10,  1.21s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:13<00:09,  1.16s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:15<00:08,  1.15s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:16<00:07,  1.24s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:17<00:05,  1.06s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:18<00:04,  1.13s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:19<00:03,  1.24s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:21<00:02,  1.23s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:21<00:01,  1.08s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:23<00:00,  1.11s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:24<00:00,  1.11s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:24<00:00,  1.23s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 24.553, 'train_samples_per_second': 13.033, 'train_steps_per_second': 0.815, 'train_loss': 0.5088278293609619, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:20,  1.07s/it] 10%|â–ˆ         | 2/20 [00:02<00:25,  1.41s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:29,  1.73s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:09<00:46,  2.93s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:11<00:37,  2.50s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:12<00:29,  2.11s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:14<00:25,  1.98s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:15<00:21,  1.82s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:17<00:18,  1.65s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:18<00:16,  1.61s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:20<00:13,  1.53s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:21<00:13,  1.66s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:23<00:11,  1.69s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:24<00:09,  1.52s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:26<00:07,  1.56s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:28<00:06,  1.66s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:29<00:04,  1.43s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:30<00:02,  1.42s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:31<00:01,  1.21s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.30s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:34<00:00,  1.30s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:34<00:00,  1.72s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 34.3806, 'train_samples_per_second': 9.308, 'train_steps_per_second': 0.582, 'train_loss': 0.48075499534606936, 'epoch': 1.0}
>> ==================== Round 30 : [1, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:24,  1.31s/it] 10%|â–ˆ         | 2/20 [00:03<00:28,  1.61s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:22,  1.32s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:21,  1.35s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:18,  1.26s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:19,  1.38s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:09<00:17,  1.32s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:11<00:17,  1.47s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:12<00:16,  1.49s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:13<00:14,  1.41s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:15<00:12,  1.38s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:16<00:10,  1.34s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:17<00:08,  1.28s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:19<00:08,  1.36s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:20<00:06,  1.24s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:21<00:05,  1.41s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:22<00:03,  1.25s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:23<00:02,  1.15s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:25<00:01,  1.30s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:26<00:00,  1.34s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.34s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:28<00:00,  1.43s/it]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 28.5464, 'train_samples_per_second': 11.21, 'train_steps_per_second': 0.701, 'train_loss': 0.46088619232177735, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:01<00:26,  1.40s/it] 10%|â–ˆ         | 2/20 [00:02<00:20,  1.13s/it] 15%|â–ˆâ–Œ        | 3/20 [00:03<00:21,  1.24s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:05<00:23,  1.48s/it] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:06<00:21,  1.42s/it] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:08<00:19,  1.37s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:09<00:17,  1.38s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:10<00:15,  1.25s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:11<00:12,  1.14s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:13<00:13,  1.34s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:13<00:09,  1.10s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:14<00:08,  1.09s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:16<00:09,  1.30s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:18<00:08,  1.38s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:18<00:05,  1.13s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:19<00:04,  1.07s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:21<00:04,  1.41s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:23<00:02,  1.34s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:24<00:01,  1.47s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:25<00:00,  1.37s/it]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.37s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:27<00:00,  1.36s/it]
{'train_runtime': 27.1549, 'train_samples_per_second': 11.784, 'train_steps_per_second': 0.737, 'train_loss': 0.5179022312164306, 'epoch': 1.0}
