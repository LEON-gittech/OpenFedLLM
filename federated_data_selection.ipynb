{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from datasets import Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_data = load_dataset(\"sahil2801/CodeAlpaca-20k\")[\"train\"]\n",
    "fin_data = load_dataset(\"FinGPT/fingpt-sentiment-train\")[\"train\"]\n",
    "med_data = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")[\"train\"]\n",
    "general_data = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n",
    "math_data = load_dataset(\"TIGER-Lab/MathInstruct\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpaca_format(example):\n",
    "    if example['input'] == \"\":\n",
    "        example[\"instruction\"] = example[\"instruction\"]\n",
    "    else:\n",
    "        example[\"instruction\"] = example[\"instruction\"] + \" \" + example['input']\n",
    "    example[\"response\"] = example['output']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sft_dataset(dataset_name, dataset, dataset_sample=None)->datasets.Dataset:\n",
    "    if dataset_name in [\"lucasmccabe-lmi/CodeAlpaca-20k\", \"yahma/alpaca-cleaned\", \"FinGPT/fingpt-sentiment-train\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"WizardLM/WizardLM_evol_instruct_70k\"]:\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif dataset_name in [\"tatsu-lab/alpaca\", \"vicgalle/alpaca-gpt4\", \"gbharti/finance-alpaca\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output', 'text'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"TIGER-Lab/MathInstruct\"]:\n",
    "        df = pd.DataFrame(dataset)\n",
    "        df = df.drop_duplicates(subset=['instruction'])\n",
    "        dataset = datasets.Dataset.from_pandas(df)\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "    elif dataset_name in [\"lighteval/MATH\"]:\n",
    "        dataset = dataset.rename_column(\"solution\", \"response\")\n",
    "        dataset = dataset.rename_column(\"problem\", \"instruction\")\n",
    "        dataset = dataset.remove_columns(['level', 'type'])\n",
    "    elif dataset_name in ['gsm8k']:\n",
    "        dataset = dataset.rename_column(\"question\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"answer\", \"response\")\n",
    "    elif dataset_name in ['medalpaca/medical_meadow_medical_flashcards']:       # TODO: 'lavita/ChatDoctor-HealthCareMagic-100k'. not sure whether to discard the instruction.\n",
    "        dataset = dataset.remove_columns(['instruction'])\n",
    "        dataset = dataset.rename_column(\"input\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif \"math\" in dataset_name:\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Dataset {dataset_name} is not supported.\")\n",
    "    dataset = dataset.shuffle(seed=2023)\n",
    "    if dataset_sample:\n",
    "        num_sample = min(len(dataset), dataset_sample)\n",
    "        dataset = dataset.select(range(num_sample))\n",
    "    print(f\">> ===== After processing, Dataset {dataset_name} has {len(dataset)} examples. =====\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ===== After processing, Dataset lucasmccabe-lmi/CodeAlpaca-20k has 20022 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset FinGPT/fingpt-sentiment-train has 76772 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset medalpaca/medical_meadow_medical_flashcards has 33955 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset tatsu-lab/alpaca has 52002 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset TIGER-Lab/MathInstruct has 224567 examples. =====\n",
      "['response', 'instruction', '__index_level_0__']\n"
     ]
    }
   ],
   "source": [
    "processed_data = []\n",
    "for name, dataset in zip([\"lucasmccabe-lmi/CodeAlpaca-20k\",\"FinGPT/fingpt-sentiment-train\",\"medalpaca/medical_meadow_medical_flashcards\",\"tatsu-lab/alpaca\",\"TIGER-Lab/MathInstruct\"],[code_data,fin_data,med_data,general_data,math_data]):\n",
    "    tmp:datasets.Dataset = process_sft_dataset(name,dataset)\n",
    "    print(tmp.column_names)\n",
    "    processed_data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concated = concatenate_datasets(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造base数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405318\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(10)\n",
    "sampled_indices = random.sample(range(len(processed_data[0])), 1000)\n",
    "sampled_data = processed_data[0].select(sampled_indices)\n",
    "sampled_set = set(sampled_indices)\n",
    "base_set = set(range(len(data_concated)))\n",
    "# 计算差集，即在 idx_set 中但不在 sampled_set 中的元素\n",
    "remaining_idx = list(base_set - sampled_set)\n",
    "print(len(remaining_idx))\n",
    "data_concated = data_concated.select(remaining_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将base数据集随机拆成十份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = sampled_data.shuffle(seed=42)  \n",
    "local_datasets = []\n",
    "for i in range(10):\n",
    "    local_datasets.append(sampled_data.shard(10, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(local_datasets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将公共数据集也随机拆成10份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concated = data_concated.shuffle(seed=42)\n",
    "public_datasets = []\n",
    "for i in range(10):\n",
    "    public_datasets.append(data_concated.shard(10,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造随机采样数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "client_random_datasets = []\n",
    "dataset: Dataset\n",
    "for dataset in public_datasets:\n",
    "    idxs = random.sample(range(len(dataset)), 5000)\n",
    "    client_random_datasets.append(dataset.select(idxs))\n",
    "print(len(client_random_datasets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e2440a1ac74f4e8a6b9d2dda7b454c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f24ddd460e499ab96672b0393d7a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93df7cc6b1c45b2a210b818f7e26dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4fe5e3a87444b29e12e713b93769d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6136bc9b90324e75919a17c4cee7ed1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea1caa31d7c4d6fbb84afa378851c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efa5faca8ca48b99716bcb34b8d378b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e8fe1083d14d13bd8ed01305ee91a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f5256cdb9c4edda585f36be0e16cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9791a16dfd8a4b05b51522ce63bcdbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, dataset in enumerate(client_random_datasets):\n",
    "    dataset = concatenate_datasets([dataset,local_datasets[i]]).shuffle(seed=42)\n",
    "    dataset.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/random_with_base_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对每一个客户端的数据集进行检索，构造 pos 和 neg 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 8*GPUs----------\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "model = FlagModel('BAAI/bge-large-en-v1.5', \n",
    "                  query_instruction_for_retrieval=\"\",\n",
    "                  use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9a606c5f5146e097f0f6a330a44969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/40532 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "public_datasets[0].save_to_disk(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 20/20 [00:19<00:00,  1.01it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:18<00:00,  1.05it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:18<00:00,  1.06it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:18<00:00,  1.05it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:19<00:00,  1.03it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:19<00:00,  1.03it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:18<00:00,  1.07it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:19<00:00,  1.04it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:19<00:00,  1.04it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:18<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "client_pos_datasets, client_neg_datasets = [], []\n",
    "k = 10\n",
    "for i, sampled_data in enumerate(local_datasets):\n",
    "    sampled_embeddings = model.encode(sampled_data[\"instruction\"])\n",
    "    # 假设 embeddings 是你的嵌入数据\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(sampled_embeddings)\n",
    "    concated_embeddings = model.encode(public_datasets[i][\"instruction\"])\n",
    "    clusters = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "    concated_embeddings = torch.tensor(concated_embeddings, dtype=torch.float32)\n",
    "    similarity_scores = clusters @ concated_embeddings.T\n",
    "    top_idxs = []\n",
    "    bot_idxs = []\n",
    "    for j in range(similarity_scores.shape[0]):\n",
    "        tmp = similarity_scores[j]\n",
    "        top_idxs.append(heapq.nlargest(500, range(len(tmp)-1), key=lambda x: tmp[x]))\n",
    "        bot_idxs.append(heapq.nsmallest(500, range(len(tmp)-1), key=lambda x: tmp[x]))\n",
    "        \n",
    "    pos_datasets: Dataset = []\n",
    "    neg_datasets: Dataset = []\n",
    "    top_idxs=np.concatenate(top_idxs,axis=None)\n",
    "    bot_idxs=np.concatenate(bot_idxs,axis=None)\n",
    "    pos_datasets = public_datasets[i].select(top_idxs)\n",
    "    neg_datasets = public_datasets[i].select(bot_idxs)\n",
    "    pos_datasets = concatenate_datasets([pos_datasets, sampled_data])\n",
    "    neg_datasets = concatenate_datasets([neg_datasets, sampled_data])\n",
    "    pos_datasets = pos_datasets.shuffle(seed=42)\n",
    "    neg_datasets = neg_datasets.shuffle(seed=42)\n",
    "    client_pos_datasets.append(pos_datasets)\n",
    "    client_neg_datasets.append(neg_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (pos_data, neg_data) in enumerate(zip(client_pos_datasets, client_neg_datasets)):\n",
    "    pos_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/pos_{i}.parquet\")\n",
    "    neg_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/neg_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造pos+diversity 数据集，一半 pos，一半 diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "client_pos_datasets=[]\n",
    "for i, sampled_data in enumerate(local_datasets):\n",
    "    sampled_embeddings = model.encode(sampled_data[\"instruction\"])\n",
    "    # 假设 embeddings 是你的嵌入数据\n",
    "    k = 10\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(sampled_embeddings)\n",
    "    concated_embeddings = model.encode(public_datasets[i][\"instruction\"])\n",
    "    clusters = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "    concated_embeddings = torch.tensor(concated_embeddings, dtype=torch.float32)\n",
    "    similarity_scores = clusters @ concated_embeddings.T\n",
    "    top_idxs = []\n",
    "    for i in range(similarity_scores.shape[0]):\n",
    "        tmp = similarity_scores[i]\n",
    "        top_idxs.append(heapq.nlargest(250, range(len(tmp)), key=tmp.__getitem__))\n",
    "    pos_datasets: Dataset = []\n",
    "    # top_idxs去重，其余作为 diversity\n",
    "    top_idxs = set(np.concatenate(top_idxs,axis=0))\n",
    "    try: top_idxs.remove(len(public_datasets[i]))\n",
    "    except: pass\n",
    "    pos_datasets = public_datasets[i].select(top_idxs)\n",
    "    print(len(top_idxs))\n",
    "    # 从public_datasets[i]中去掉 top_idxs\n",
    "    all_idxs = set(range(len(public_datasets[i])))\n",
    "    remain_idxs = list(all_idxs-top_idxs)\n",
    "    random_idxs = random.sample(remain_idxs, 5000-len(top_idxs))\n",
    "    diversity_datasets = public_datasets[i].select(random_idxs)\n",
    "    pos_datasets = concatenate_datasets([pos_datasets, diversity_datasets, sampled_data])\n",
    "    pos_datasets = pos_datasets.shuffle(seed=42)\n",
    "    client_pos_datasets.append(pos_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pos_data in enumerate(client_pos_datasets):\n",
    "    pos_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/T_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造去重 pos 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedSet([3, 7, 1])\n"
     ]
    }
   ],
   "source": [
    "from ordered_set import OrderedSet\n",
    "tmp = OrderedSet([4,5,3,7,1])\n",
    "tmp1 = OrderedSet([4,5])\n",
    "for t in tmp1: tmp.discard(t)\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "client_pos_datasets=[]\n",
    "for i, sampled_data in enumerate(local_datasets):\n",
    "    sampled_embeddings = model.encode(sampled_data[\"instruction\"])\n",
    "    # 假设 embeddings 是你的嵌入数据\n",
    "    k = 10\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(sampled_embeddings)\n",
    "    concated_embeddings = model.encode(public_datasets[i][\"instruction\"])\n",
    "    clusters = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "    concated_embeddings = torch.tensor(concated_embeddings, dtype=torch.float32)\n",
    "    top_idxs:OrderedSet = OrderedSet()\n",
    "    remain_idxs = OrderedSet(range(len(public_datasets[i])))\n",
    "    for j in range(k):\n",
    "        similarity_scores = clusters[j] @ concated_embeddings.T\n",
    "        top_idx = list(OrderedSet(heapq.nlargest(5000, range(len(similarity_scores)-1), key=lambda x: similarity_scores[x]))-top_idxs)[:500]\n",
    "        top_idxs.update(top_idx)\n",
    "        print(\"top_idxs\", len(top_idxs))\n",
    "        remain_idxs.difference_update(top_idx)\n",
    "        print(\"remain_idxs\", len(remain_idxs))\n",
    "\n",
    "    pos_datasets = public_datasets[i].select(list(top_idxs))\n",
    "    pos_datasets = concatenate_datasets([pos_datasets, sampled_data])\n",
    "    pos_datasets = pos_datasets.shuffle(seed=42)\n",
    "    client_pos_datasets.append(pos_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fffa476247f462789ce951817bbcb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a907969aff748f189976637565ad697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff285a5a70d49f688c5a8f336662bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d97b04c54ad40c8b3d6a8199949c2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32443e846e040c6b7fac882492c5dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a57afc5a8144a3acea09a242574317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe183f575f2b4d13870f8b333cffae6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dca318328bd418194380737840dd6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d0c1af593644608f515bea7173fa6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd4cbaece7f46d3b0d2a3575c00167c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, pos_data in enumerate(client_pos_datasets):\n",
    "    pos_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/pos_nodup_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
