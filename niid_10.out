/home/tiger/.local/lib/python3.9/site-packages/bytedmetrics/__init__.py:10: UserWarning: bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`
  warnings.warn("bytedmetrics is renamed to bytedance.metrics, please using `bytedance.metrics` instead of `bytedmetrics`")
[2024-07-30 19:44:27,802] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ScriptArguments(model_name_or_path='/mnt/bn/data-tns-live-llm/leon/datasets/llama-3-8b-bnb-4bit/', dataset_name='niid_med_10', log_with='none', learning_rate=5e-05, batch_size=16, seq_length=2048, gradient_accumulation_steps=1, load_in_8bit=False, load_in_4bit=True, use_peft=True, trust_remote_code=False, output_dir='/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_med_10_20000_fedavg_c10s2_i20_b16a1_l2048_r128a256_f0', peft_lora_r=128, peft_lora_alpha=256, logging_steps=100, use_auth_token=False, num_train_epochs=5, max_steps=20, save_steps=1000, save_total_limit=3, push_to_hub=False, hub_model_id=None, gradient_checkpointing=True, template='alpaca', seed=2023, dpo_beta=0.1, dataset_sample=20000, local_data_dir=None, unsloth=1, bf16=1, online_dataset=0, full_data=0) FedArguments(fed_alg='fedavg', num_rounds=30, num_clients=10, sample_clients=2, split_strategy='iid', prox_mu=0.01, fedopt_tau=0.001, fedopt_eta=0.001, fedopt_beta1=0.9, fedopt_beta2=0.99, save_model_freq=10)
using unsloth model
==((====))==  Unsloth: Fast Llama patching release 2024.8
   \\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.109 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 9.0. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
30
>> ==================== Round 1 : [6, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:02<00:44,  2.36s/it] 10%|â–ˆ         | 2/20 [00:03<00:30,  1.69s/it] 15%|â–ˆâ–Œ        | 3/20 [00:04<00:20,  1.20s/it] 20%|â–ˆâ–ˆ        | 4/20 [00:04<00:15,  1.06it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:05<00:13,  1.15it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:06<00:11,  1.27it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:06<00:09,  1.37it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:07<00:08,  1.49it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:07<00:07,  1.57it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:08<00:06,  1.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:08<00:05,  1.73it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:09<00:04,  1.73it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:10<00:04,  1.69it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:10<00:03,  1.68it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:11<00:02,  1.71it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:11<00:02,  1.79it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:12<00:01,  1.75it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:12<00:01,  1.71it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:13<00:00,  1.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.81it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:19<00:00,  1.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:19<00:00,  1.05it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 19.1329, 'train_samples_per_second': 16.725, 'train_steps_per_second': 1.045, 'train_loss': 0.7608850955963135, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.53it/s] 10%|â–ˆ         | 2/20 [00:01<00:12,  1.44it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.58it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.56it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.63it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:09,  1.54it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.68it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.73it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.76it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:05,  1.77it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.72it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.81it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.81it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.79it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.84it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.89it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.90it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.88it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.85it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.79it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.79it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.54it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.0191, 'train_samples_per_second': 24.579, 'train_steps_per_second': 1.536, 'train_loss': 0.7982414245605469, 'epoch': 1.0}
>> ==================== Round 2 : [1, 2] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.63it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.77it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.78it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.71it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.77it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.67it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.66it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.71it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.66it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:06,  1.66it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.67it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.71it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.77it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.72it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.82it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.76it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.78it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.82it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.18it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.8869, 'train_samples_per_second': 18.95, 'train_steps_per_second': 1.184, 'train_loss': 0.7093065261840821, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.76it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.76it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  1.91it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.94it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:07,  1.95it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.97it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:06,  1.86it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.89it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.94it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.94it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:04,  1.91it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.89it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:03,  1.95it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.94it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:07<00:02,  1.94it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.89it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.78it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.82it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.70it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.70it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.57it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.7413, 'train_samples_per_second': 25.115, 'train_steps_per_second': 1.57, 'train_loss': 0.7024177551269531, 'epoch': 1.0}
>> ==================== Round 3 : [0, 1] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.62it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.80it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.86it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.78it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.86it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.74it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.68it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.78it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.93it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.99it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:04,  1.98it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.97it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:03,  2.00it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.79it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.79it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.82it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.85it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.85it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.88it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.86it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.86it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.24it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.1603, 'train_samples_per_second': 19.802, 'train_steps_per_second': 1.238, 'train_loss': 0.6994355201721192, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.65it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.75it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  1.97it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.91it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:07,  1.96it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.71it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.81it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.93it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  2.01it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.98it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:04,  1.93it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.92it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:03,  1.96it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.90it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:07<00:02,  1.95it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.85it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:08<00:01,  1.99it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.93it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.84it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.90it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.90it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.62it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.3545, 'train_samples_per_second': 25.901, 'train_steps_per_second': 1.619, 'train_loss': 0.6937284946441651, 'epoch': 1.0}
>> ==================== Round 4 : [3, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.76it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.79it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.78it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.78it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.72it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.78it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.71it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.79it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:05,  1.84it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.69it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.75it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.83it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.81it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.87it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.86it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.86it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.85it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.83it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.85it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.84it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.84it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.57it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.7336, 'train_samples_per_second': 25.13, 'train_steps_per_second': 1.571, 'train_loss': 0.6871823310852051, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.53it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.70it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.78it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.77it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.81it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.82it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.85it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.87it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.85it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.84it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.87it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.84it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.88it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.85it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.83it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.84it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.83it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.84it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.86it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.88it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.88it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.59it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.5904, 'train_samples_per_second': 25.416, 'train_steps_per_second': 1.589, 'train_loss': 0.7160743713378906, 'epoch': 1.0}
>> ==================== Round 5 : [3, 4] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.70it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.82it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.82it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.85it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.87it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.76it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.83it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.85it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.89it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.84it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.83it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.80it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.70it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.72it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:03,  1.66it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.50it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.60it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.59it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.65it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.58it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:17<00:00,  1.12it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 17.7952, 'train_samples_per_second': 17.982, 'train_steps_per_second': 1.124, 'train_loss': 0.6552905082702637, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.89it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.81it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.84it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.82it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:07,  1.94it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.96it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:06,  1.97it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.98it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.90it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.91it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:04,  1.88it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.88it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:03,  1.88it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.84it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:07<00:02,  1.85it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.94it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:08<00:01,  1.92it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.81it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.76it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.48it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.5388, 'train_samples_per_second': 23.636, 'train_steps_per_second': 1.477, 'train_loss': 0.6897051334381104, 'epoch': 1.0}
>> ==================== Round 6 : [4, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.65it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.84it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  1.93it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.87it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.81it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.84it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:06,  1.89it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.92it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.97it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.84it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:04,  1.84it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.81it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.84it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.82it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.85it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.87it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.78it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.72it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.80it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.88it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.88it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.59it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.5852, 'train_samples_per_second': 25.427, 'train_steps_per_second': 1.589, 'train_loss': 0.6878520965576171, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.81it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.77it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.80it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.85it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.84it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.87it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:06,  1.94it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.95it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.88it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.85it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:05,  1.79it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.71it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.66it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.72it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.74it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.79it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.78it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.78it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.82it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.56it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.7955, 'train_samples_per_second': 25.009, 'train_steps_per_second': 1.563, 'train_loss': 0.6747699737548828, 'epoch': 1.0}
>> ==================== Round 7 : [1, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:09,  1.92it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.85it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.76it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.81it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.83it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.72it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.77it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.71it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.67it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.73it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.78it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.71it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.69it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.67it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.69it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.72it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.75it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.70it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.76it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.51it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.2656, 'train_samples_per_second': 24.123, 'train_steps_per_second': 1.508, 'train_loss': 0.6434895038604737, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.72it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.67it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.85it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.82it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.82it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.81it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:06,  1.86it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.88it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.84it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.78it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.76it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.86it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.84it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.86it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.85it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.84it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.87it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.90it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.87it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.77it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.65it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.142, 'train_samples_per_second': 26.355, 'train_steps_per_second': 1.647, 'train_loss': 0.6826938629150391, 'epoch': 1.0}
>> ==================== Round 8 : [2, 5] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.69it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.79it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.84it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.87it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.86it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.88it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:06,  1.86it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.78it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:06,  1.82it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.81it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.83it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.72it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.78it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.82it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.85it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.84it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.76it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.77it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.79it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.79it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.59it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.5928, 'train_samples_per_second': 25.411, 'train_steps_per_second': 1.588, 'train_loss': 0.6856013298034668, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.40it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.62it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.62it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.60it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.64it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.76it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.76it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.75it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.70it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.79it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.78it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.73it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.65it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.71it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.77it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.87it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.88it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.79it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.79it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:16<00:00,  1.22it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 16.3479, 'train_samples_per_second': 19.574, 'train_steps_per_second': 1.223, 'train_loss': 0.6791229248046875, 'epoch': 1.0}
>> ==================== Round 9 : [3, 5] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.55it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.73it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.84it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.85it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.74it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.78it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.77it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.76it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.71it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.72it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.77it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.68it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:03,  1.65it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.70it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.54it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.61it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.67it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.53it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.0592, 'train_samples_per_second': 24.504, 'train_steps_per_second': 1.531, 'train_loss': 0.6499309539794922, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.32it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.62it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.70it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.67it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.75it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.77it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.84it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.87it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:05,  1.86it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:04,  2.12it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:04,  1.98it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:03,  2.02it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:03,  2.01it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.88it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.85it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.76it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.69it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.75it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.80it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.80it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.45it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.8293, 'train_samples_per_second': 23.139, 'train_steps_per_second': 1.446, 'train_loss': 0.6790663242340088, 'epoch': 1.0}
>> ==================== Round 10 : [5, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.65it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.72it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.66it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.72it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.78it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.83it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:06,  1.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.91it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.80it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.79it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.78it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.82it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.84it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.87it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.77it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.77it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.77it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.79it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.73it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.80it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.80it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.49it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.3856, 'train_samples_per_second': 23.906, 'train_steps_per_second': 1.494, 'train_loss': 0.6653265476226806, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.76it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.79it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.69it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.83it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.84it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.89it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.80it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.81it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:06,  1.79it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.73it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.68it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.71it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.74it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.85it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.83it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.85it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.84it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.83it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.81it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.39it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.4372, 'train_samples_per_second': 22.165, 'train_steps_per_second': 1.385, 'train_loss': 0.6711334705352783, 'epoch': 1.0}
>> ==================== Round 11 : [0, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.52it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.67it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.62it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.70it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.65it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.68it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.70it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.68it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.65it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.69it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.68it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.66it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.72it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.77it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.71it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.74it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.82it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.87it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.84it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.84it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.56it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.8138, 'train_samples_per_second': 24.973, 'train_steps_per_second': 1.561, 'train_loss': 0.6370153427124023, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.55it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.72it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.80it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.74it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.76it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.80it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.77it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.81it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.79it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.77it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.81it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.89it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.86it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.82it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.87it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.80it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.83it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.82it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.73it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.83it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.83it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.44it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.874, 'train_samples_per_second': 23.065, 'train_steps_per_second': 1.442, 'train_loss': 0.670068359375, 'epoch': 1.0}
>> ==================== Round 12 : [7, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.42it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.51it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.63it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.72it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:08,  1.72it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.74it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.79it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.69it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.72it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.67it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.65it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.69it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.75it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.76it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.76it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.75it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.78it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.87it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.88it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.88it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.36it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.7585, 'train_samples_per_second': 21.682, 'train_steps_per_second': 1.355, 'train_loss': 0.6466997623443603, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.70it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.75it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.83it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.80it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.80it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.87it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.79it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.75it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:06,  1.82it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.82it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.76it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.81it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.79it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.80it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.67it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.71it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.76it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.80it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.89it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.86it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.86it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.55it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.8738, 'train_samples_per_second': 24.857, 'train_steps_per_second': 1.554, 'train_loss': 0.6997508049011231, 'epoch': 1.0}
>> ==================== Round 13 : [4, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.35it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.55it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.64it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.64it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:08,  1.75it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.83it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.82it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.84it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.74it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.80it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.79it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.72it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.81it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.80it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.73it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.74it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.71it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.67it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.66it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.68it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.68it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:18<00:00,  1.09it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 18.2798, 'train_samples_per_second': 17.506, 'train_steps_per_second': 1.094, 'train_loss': 0.6494291305541993, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.51it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.68it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.71it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.73it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.70it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.82it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.74it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.79it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.73it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.74it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.81it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.79it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.82it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.82it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.80it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.77it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.80it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.78it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.57it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.7043, 'train_samples_per_second': 25.188, 'train_steps_per_second': 1.574, 'train_loss': 0.6642699241638184, 'epoch': 1.0}
>> ==================== Round 14 : [4, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.90it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.72it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.65it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.70it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:09,  1.67it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.71it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.68it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.77it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.76it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.72it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.73it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.83it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.81it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.80it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.79it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.78it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.84it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.75it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.80it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.79it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.79it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.36it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.6693, 'train_samples_per_second': 21.814, 'train_steps_per_second': 1.363, 'train_loss': 0.6746280670166016, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.71it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.75it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.75it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.82it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.85it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.88it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:06,  1.89it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.87it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.86it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.84it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.82it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.80it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.88it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.82it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.88it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.90it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.81it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.88it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.85it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.85it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.51it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.2838, 'train_samples_per_second': 24.089, 'train_steps_per_second': 1.506, 'train_loss': 0.6600651264190673, 'epoch': 1.0}
>> ==================== Round 15 : [1, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:15,  1.20it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.50it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.61it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.77it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.78it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.82it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.83it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.81it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.72it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.82it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.81it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.80it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.71it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.74it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.75it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.78it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.71it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.68it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.80it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.80it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.47it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.5935, 'train_samples_per_second': 23.541, 'train_steps_per_second': 1.471, 'train_loss': 0.6417880535125733, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.81it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.89it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.86it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.84it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:07,  1.90it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.78it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.85it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.87it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.93it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.84it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:04,  1.87it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.91it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:03,  1.96it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.89it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.84it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:01,  2.00it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.92it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.78it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.79it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.79it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.39it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.3891, 'train_samples_per_second': 22.239, 'train_steps_per_second': 1.39, 'train_loss': 0.6631971836090088, 'epoch': 1.0}
>> ==================== Round 16 : [0, 3] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.77it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.91it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  1.92it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.81it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.82it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.74it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.75it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.75it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.78it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.77it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.79it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.79it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.80it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.78it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.84it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.82it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.74it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.70it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.83it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.83it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.58it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.6769, 'train_samples_per_second': 25.243, 'train_steps_per_second': 1.578, 'train_loss': 0.6340795516967773, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.40it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.67it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.56it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:10,  1.56it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.66it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.72it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.77it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.69it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.73it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:06<00:06,  1.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.70it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.79it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.80it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.79it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.80it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.89it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.85it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.89it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.92it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.91it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.91it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.52it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.2047, 'train_samples_per_second': 24.234, 'train_steps_per_second': 1.515, 'train_loss': 0.625372838973999, 'epoch': 1.0}
>> ==================== Round 17 : [5, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.49it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.52it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.68it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.71it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.61it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.63it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.71it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.77it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.74it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.79it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.72it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.74it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.77it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.71it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.82it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.74it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.76it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.80it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.83it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.80it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.80it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.51it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.2864, 'train_samples_per_second': 24.085, 'train_steps_per_second': 1.505, 'train_loss': 0.6362347602844238, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.53it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.78it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.81it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.75it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.76it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.81it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.81it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.87it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.77it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.77it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.85it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.89it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.81it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.85it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.83it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.77it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.74it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.74it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.74it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.33it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.0641, 'train_samples_per_second': 21.242, 'train_steps_per_second': 1.328, 'train_loss': 0.6220116138458252, 'epoch': 1.0}
>> ==================== Round 18 : [6, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.50it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.59it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.67it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.63it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.72it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.79it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.80it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.86it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.76it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.78it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.81it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.80it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.85it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.79it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.86it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.85it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.83it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.88it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.73it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.73it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.56it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.8485, 'train_samples_per_second': 24.906, 'train_steps_per_second': 1.557, 'train_loss': 0.6365589141845703, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.67it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.74it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.74it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.77it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.78it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.65it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:08,  1.58it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.77it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.78it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.79it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.75it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.83it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.82it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.81it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.76it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.77it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.76it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.89it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.89it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.55it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.9239, 'train_samples_per_second': 24.76, 'train_steps_per_second': 1.548, 'train_loss': 0.6626266479492188, 'epoch': 1.0}
>> ==================== Round 19 : [1, 2] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.48it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.67it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.81it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.72it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.76it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.79it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.83it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.87it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:05,  1.85it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.88it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.76it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.76it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.77it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.74it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.70it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.79it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.82it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.82it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.84it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.83it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.83it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.55it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.8779, 'train_samples_per_second': 24.849, 'train_steps_per_second': 1.553, 'train_loss': 0.6311729431152344, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.41it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.64it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.72it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.75it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:03<00:09,  1.66it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.68it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.71it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.71it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.72it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.73it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.83it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.83it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.84it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.82it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.82it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.83it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.91it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.96it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.87it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.78it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.55it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.8805, 'train_samples_per_second': 24.844, 'train_steps_per_second': 1.553, 'train_loss': 0.6432084560394287, 'epoch': 1.0}
>> ==================== Round 20 : [0, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.55it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.79it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.86it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.74it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.80it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.83it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.75it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.69it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.75it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.81it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.84it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.87it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.88it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.77it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.81it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.68it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.71it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.68it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.72it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.74it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.74it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.56it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.8138, 'train_samples_per_second': 24.973, 'train_steps_per_second': 1.561, 'train_loss': 0.6271289825439453, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.68it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.89it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.76it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.65it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.70it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.65it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.63it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.76it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.81it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.87it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.80it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.81it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.76it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.73it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.75it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.65it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.72it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.77it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.81it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.54it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.0178, 'train_samples_per_second': 24.582, 'train_steps_per_second': 1.536, 'train_loss': 0.6434636592864991, 'epoch': 1.0}
>> ==================== Round 21 : [2, 4] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.71it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.67it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.77it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.77it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.76it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.80it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.81it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.86it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.94it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.81it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.90it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.85it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.88it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.93it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.91it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.85it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.86it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.77it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.77it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.39it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.3937, 'train_samples_per_second': 22.232, 'train_steps_per_second': 1.389, 'train_loss': 0.6235729694366455, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.82it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.77it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.68it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.64it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:09,  1.64it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.68it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.65it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.61it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.69it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.71it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.67it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.75it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.71it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:10<00:01,  1.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.74it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:11<00:00,  1.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.74it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.74it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.26it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.8985, 'train_samples_per_second': 20.128, 'train_steps_per_second': 1.258, 'train_loss': 0.6239154815673829, 'epoch': 1.0}
>> ==================== Round 22 : [2, 6] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:12,  1.54it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.66it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.66it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.69it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.76it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.78it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.76it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.81it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.78it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.77it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.78it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.72it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.77it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.84it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.82it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.80it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.83it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.82it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.58it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.6629, 'train_samples_per_second': 25.271, 'train_steps_per_second': 1.579, 'train_loss': 0.6271521091461182, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.77it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.91it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.87it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.75it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.77it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.79it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.78it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.79it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.86it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.72it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.79it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.78it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.69it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.74it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.75it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.68it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.55it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.67it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.71it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.71it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.54it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.9607, 'train_samples_per_second': 24.69, 'train_steps_per_second': 1.543, 'train_loss': 0.6333516597747803, 'epoch': 1.0}
>> ==================== Round 23 : [2, 3] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.72it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.77it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.78it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.86it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.78it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.69it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.68it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.71it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.74it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.75it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.79it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.72it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.78it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:03,  1.64it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.62it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.70it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.72it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.78it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.57it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.7054, 'train_samples_per_second': 25.186, 'train_steps_per_second': 1.574, 'train_loss': 0.6238903999328613, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.43it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.61it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.67it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.73it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.80it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.80it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.75it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.73it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:06,  1.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.66it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:07<00:04,  1.70it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.68it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.76it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.80it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.79it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.88it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.90it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.72it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.77it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.49it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.3978, 'train_samples_per_second': 23.885, 'train_steps_per_second': 1.493, 'train_loss': 0.6207464218139649, 'epoch': 1.0}
>> ==================== Round 24 : [1, 4] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.74it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.88it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.89it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.79it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.80it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.79it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.82it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.73it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.67it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.82it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.85it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.87it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.85it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.83it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.82it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.84it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.81it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.71it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.75it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.75it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.56it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.8105, 'train_samples_per_second': 24.979, 'train_steps_per_second': 1.561, 'train_loss': 0.6053103446960449, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.76it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.80it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.88it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.90it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.85it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.87it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:06,  1.89it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.88it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.89it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.91it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:04,  1.85it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.87it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:03,  1.88it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.85it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.74it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.80it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.85it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.77it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.76it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.62it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.3544, 'train_samples_per_second': 25.902, 'train_steps_per_second': 1.619, 'train_loss': 0.6447600364685059, 'epoch': 1.0}
>> ==================== Round 25 : [2, 6] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.68it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.74it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.74it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.94it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.77it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.80it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.72it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.74it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.79it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.78it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.82it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.83it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.89it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.77it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.67it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.70it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.68it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.68it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.68it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.48it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.54, 'train_samples_per_second': 23.634, 'train_steps_per_second': 1.477, 'train_loss': 0.6306230545043945, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:14,  1.35it/s] 10%|â–ˆ         | 2/20 [00:01<00:11,  1.62it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.74it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.81it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.72it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.66it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.69it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.80it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.80it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.84it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.77it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.71it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.73it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.74it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.75it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.74it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.81it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.75it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.70it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.70it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.40it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.3361, 'train_samples_per_second': 22.321, 'train_steps_per_second': 1.395, 'train_loss': 0.6325919151306152, 'epoch': 1.0}
>> ==================== Round 26 : [0, 6] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.63it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.78it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:10,  1.67it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.76it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.76it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.78it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.73it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.78it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.79it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.74it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.75it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.74it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.71it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.72it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.74it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.75it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.69it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.71it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.77it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.77it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.54it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 13.0298, 'train_samples_per_second': 24.559, 'train_steps_per_second': 1.535, 'train_loss': 0.6430493354797363, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.67it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.73it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.81it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.83it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.77it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.84it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.72it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.66it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:06,  1.65it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.69it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.75it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.69it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.67it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.69it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.74it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.72it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.72it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.34it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.936, 'train_samples_per_second': 21.425, 'train_steps_per_second': 1.339, 'train_loss': 0.6273898601531982, 'epoch': 1.0}
>> ==================== Round 27 : [3, 9] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.42it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.67it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.76it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.65it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.70it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:08,  1.73it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.73it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.76it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.70it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.79it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.79it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.79it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.82it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.84it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.81it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.83it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.80it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.78it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.74it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.76it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.30it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.3817, 'train_samples_per_second': 20.804, 'train_steps_per_second': 1.3, 'train_loss': 0.626899528503418, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.41it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.67it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.80it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.83it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:07,  1.91it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.91it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.67it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.76it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.78it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.72it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.74it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.65it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.68it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.72it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.75it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.75it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.79it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.79it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.57it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.7193, 'train_samples_per_second': 25.159, 'train_steps_per_second': 1.572, 'train_loss': 0.6383041858673095, 'epoch': 1.0}
>> ==================== Round 28 : [4, 7] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.79it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.84it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.81it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.83it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.80it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.79it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.71it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.73it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.81it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.86it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.88it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.83it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.88it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.78it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.84it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.83it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.85it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.89it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.74it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.78it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.28it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.6158, 'train_samples_per_second': 20.492, 'train_steps_per_second': 1.281, 'train_loss': 0.6319701194763183, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:13,  1.46it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.64it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.71it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.71it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.74it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.76it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.77it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.77it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.83it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.91it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.86it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.85it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.80it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.75it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.77it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.76it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.73it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.75it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.76it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:12<00:00,  1.58it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 12.6746, 'train_samples_per_second': 25.247, 'train_steps_per_second': 1.578, 'train_loss': 0.6098939418792725, 'epoch': 1.0}
>> ==================== Round 29 : [1, 2] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:10,  1.75it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.83it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.87it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.67it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.69it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.76it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:04<00:07,  1.69it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:07,  1.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.64it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.74it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:05,  1.75it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.76it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.75it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:08<00:03,  1.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.80it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.83it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.72it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.75it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.62it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.62it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.31it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.246, 'train_samples_per_second': 20.989, 'train_steps_per_second': 1.312, 'train_loss': 0.6239605903625488, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.63it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.82it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  1.90it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.85it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:07,  1.95it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.88it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.85it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.90it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.91it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.90it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:04,  1.85it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.83it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:04,  1.74it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.67it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.79it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.79it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.80it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.75it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.71it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.72it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.72it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:15<00:00,  1.26it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 15.8368, 'train_samples_per_second': 20.206, 'train_steps_per_second': 1.263, 'train_loss': 0.6220149517059326, 'epoch': 1.0}
>> ==================== Round 30 : [1, 8] ====================
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:09,  1.93it/s] 10%|â–ˆ         | 2/20 [00:01<00:09,  1.82it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:08,  1.89it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:09,  1.75it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.86it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.92it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:06,  1.97it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.95it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  1.94it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.90it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:04,  1.86it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.93it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:03,  1.88it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.86it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:07<00:02,  1.93it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.89it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.81it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.83it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.83it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.85it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.85it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:14<00:00,  1.35it/s]
WARNING:root:Formatting inputs...
WARNING:root:Tokenizing inputs... This may take some time...
max_steps is given, it will override any value given in num_train_epochs
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 320 | Num Epochs = 1
O^O/ \_/ \    Batch size per device = 16 | Gradient Accumulation steps = 1
\        /    Total batch size = 16 | Total steps = 20
 "-____-"     Number of trainable parameters = 41,943,040
{'train_runtime': 14.8592, 'train_samples_per_second': 21.535, 'train_steps_per_second': 1.346, 'train_loss': 0.6318002700805664, 'epoch': 1.0}
  0%|          | 0/20 [00:00<?, ?it/s]  5%|â–Œ         | 1/20 [00:00<00:11,  1.65it/s] 10%|â–ˆ         | 2/20 [00:01<00:10,  1.73it/s] 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.77it/s] 20%|â–ˆâ–ˆ        | 4/20 [00:02<00:08,  1.82it/s] 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:02<00:08,  1.80it/s] 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:03<00:07,  1.80it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:03<00:07,  1.79it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:04<00:06,  1.83it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:05<00:06,  1.76it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:05,  1.78it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:06<00:04,  1.85it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:04,  1.87it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:07<00:03,  1.87it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:07<00:03,  1.71it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:08<00:02,  1.68it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:09<00:02,  1.70it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:09<00:01,  1.67it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:10<00:01,  1.71it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.74it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.81it/s]                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.43it/s]
{'train_runtime': 13.9674, 'train_samples_per_second': 22.911, 'train_steps_per_second': 1.432, 'train_loss': 0.6458628654479981, 'epoch': 1.0}
